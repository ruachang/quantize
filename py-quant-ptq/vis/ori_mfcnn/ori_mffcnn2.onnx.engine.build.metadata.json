{"model_options": {"Format": "ONNX", "Model": "/home/developers/liuchang/quantize/py-quant-ptq/ori_mffcnn2.onnx"}, "build_options": {"Max batch": "explicit batch", "Memory Pools": "workspace", "minTiming": "1", "avgTiming": "8", "Precision": "FP32"}}