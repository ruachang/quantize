Engine Layer Information:
Layer(NoOp): Reformatting CopyNode for Network Input input, Tactic: 0x0000000000000000, input (Float[1,1,60,60]) -> Reformatted input (Float[1,1,60,60])
Layer(Reformat): QuantizeLinear_206, Tactic: 0x0000000000000000, Reformatted input (Float[1,1,60,60]) -> 309 (Int8[1,1,60,60])
Layer(Reformat): QuantizeLinear_2, Tactic: 0x00000000000003e8, Reformatted input (Float[1,1,60,60]) -> 109 (Int8[1,1:32,60,60])
Layer(Reformat): Slice_203, Tactic: 0x00000000000003e8, 309 (Int8[1,1,24,24]) -> 317 (Int8[1,1:32,24,24])
Layer(Reformat): Slice_352, Tactic: 0x0000000000000000, 309 (Int8[1,1,24,24]) -> 466 (Int8[1,1:32,24,24])
Layer(Reformat): Slice_502, Tactic: 0x00000000000003e8, 309 (Int8[1,1,24,24]) -> 616 (Int8[1,1:4,24,24])
Layer(Reformat): Slice_652, Tactic: 0x00000000000003e8, 309 (Int8[1,1,24,24]) -> 766 (Int8[1,1:32,24,24])
Layer(Reformat): Slice_802, Tactic: 0x00000000000003e8, 309 (Int8[1,1,24,24]) -> 916 (Int8[1,1:32,24,24])
Layer(Reformat): Slice_952, Tactic: 0x0000000000000000, 309 (Int8[1,1,24,24]) -> 1066 (Int8[1,1:32,24,24])
Layer(Reformat): Slice_1102, Tactic: 0x00000000000003e8, 309 (Int8[1,1,24,24]) -> 1216 (Int8[1,1:32,24,24])
Layer(Reformat): Slice_1252, Tactic: 0x0000000000000000, 309 (Int8[1,1,24,24]) -> 1366 (Int8[1,1:32,24,24])
Layer(Reformat): Slice_1402, Tactic: 0x00000000000003e8, 309 (Int8[1,1,24,24]) -> 1516 (Int8[1,1:32,24,24])
Layer(Reformat): Slice_1552, Tactic: 0x00000000000003e8, 309 (Int8[1,1,24,24]) -> 1666 (Int8[1,1:32,24,24])
Layer(Reformat): Slice_1702, Tactic: 0x0000000000000000, 309 (Int8[1,1,24,24]) -> 1816 (Int8[1,1:4,24,24])
Layer(Reformat): Slice_1852, Tactic: 0x00000000000003e8, 309 (Int8[1,1,24,24]) -> 1966 (Int8[1,1:32,24,24])
Layer(Reformat): Slice_2002, Tactic: 0x0000000000000000, 309 (Int8[1,1,24,24]) -> 2116 (Int8[1,1:32,24,24])
Layer(Reformat): Slice_2152, Tactic: 0x0000000000000000, 309 (Int8[1,1,24,24]) -> 2266 (Int8[1,1:32,24,24])
Layer(Reformat): Slice_2302, Tactic: 0x00000000000003e8, 309 (Int8[1,1,24,24]) -> 2416 (Int8[1,1:32,24,24])
Layer(Reformat): Slice_2452, Tactic: 0x00000000000003e8, 309 (Int8[1,1,24,24]) -> 2566 (Int8[1,1:32,24,24])
Layer(CaskConvolution): conv1.0.weight + QuantizeLinear_8 + Conv_12, Tactic: 0xbb88763c3b0e94d4, 109 (Int8[1,1:32,60,60]) -> 124 (Int8[1,64:32,58,58])
Layer(CaskConvolution): conv4.0.weight_clone_0 + QuantizeLinear_212 + Conv_216, Tactic: 0xd3f592fae61c7986, 317 (Int8[1,1:32,24,24]) -> 332 (Int8[1,64:32,22,22])
Layer(CaskConvolution): conv4.0.weight_clone_1 + QuantizeLinear_361 + Conv_365, Tactic: 0xd3f592fae61c7986, 466 (Int8[1,1:32,24,24]) -> 481 (Int8[1,64:32,22,22])
Layer(CaskConvolution): conv4.0.weight_clone_2 + QuantizeLinear_511 + Conv_515, Tactic: 0x9fc2bcaa51428a78, 616 (Int8[1,1:4,24,24]) -> 631 (Int8[1,64:32,22,22])
Layer(CaskConvolution): conv4.0.weight_clone_3 + QuantizeLinear_661 + Conv_665, Tactic: 0xd3f592fae61c7986, 766 (Int8[1,1:32,24,24]) -> 781 (Int8[1,64:32,22,22])
Layer(CaskConvolution): conv4.0.weight_clone_4 + QuantizeLinear_811 + Conv_815, Tactic: 0xd3f592fae61c7986, 916 (Int8[1,1:32,24,24]) -> 931 (Int8[1,64:32,22,22])
Layer(CaskConvolution): conv4.0.weight_clone_5 + QuantizeLinear_961 + Conv_965, Tactic: 0xd3f592fae61c7986, 1066 (Int8[1,1:32,24,24]) -> 1081 (Int8[1,64:32,22,22])
Layer(CaskConvolution): conv4.0.weight_clone_6 + QuantizeLinear_1111 + Conv_1115, Tactic: 0xd3f592fae61c7986, 1216 (Int8[1,1:32,24,24]) -> 1231 (Int8[1,64:32,22,22])
Layer(CaskConvolution): conv4.0.weight_clone_7 + QuantizeLinear_1261 + Conv_1265, Tactic: 0xd3f592fae61c7986, 1366 (Int8[1,1:32,24,24]) -> 1381 (Int8[1,64:32,22,22])
Layer(CaskConvolution): conv4.0.weight_clone_8 + QuantizeLinear_1411 + Conv_1415, Tactic: 0xd3f592fae61c7986, 1516 (Int8[1,1:32,24,24]) -> 1531 (Int8[1,64:32,22,22])
Layer(CaskConvolution): conv4.0.weight_clone_9 + QuantizeLinear_1561 + Conv_1565, Tactic: 0xd3f592fae61c7986, 1666 (Int8[1,1:32,24,24]) -> 1681 (Int8[1,64:32,22,22])
Layer(CaskConvolution): conv4.0.weight_clone_10 + QuantizeLinear_1711 + Conv_1715, Tactic: 0x9fc2bcaa51428a78, 1816 (Int8[1,1:4,24,24]) -> 1831 (Int8[1,64:32,22,22])
Layer(CaskConvolution): conv4.0.weight_clone_11 + QuantizeLinear_1861 + Conv_1865, Tactic: 0xd3f592fae61c7986, 1966 (Int8[1,1:32,24,24]) -> 1981 (Int8[1,64:32,22,22])
Layer(CaskConvolution): conv4.0.weight_clone_12 + QuantizeLinear_2011 + Conv_2015, Tactic: 0xd3f592fae61c7986, 2116 (Int8[1,1:32,24,24]) -> 2131 (Int8[1,64:32,22,22])
Layer(CaskConvolution): conv4.0.weight_clone_13 + QuantizeLinear_2161 + Conv_2165, Tactic: 0xd3f592fae61c7986, 2266 (Int8[1,1:32,24,24]) -> 2281 (Int8[1,64:32,22,22])
Layer(CaskConvolution): conv4.0.weight_clone_14 + QuantizeLinear_2311 + Conv_2315, Tactic: 0xd3f592fae61c7986, 2416 (Int8[1,1:32,24,24]) -> 2431 (Int8[1,64:32,22,22])
Layer(CaskConvolution): conv4.0.weight_clone_15 + QuantizeLinear_2461 + Conv_2465, Tactic: 0xd3f592fae61c7986, 2566 (Int8[1,1:32,24,24]) -> 2581 (Int8[1,64:32,22,22])
Layer(CaskConvolution): conv1.3.weight + QuantizeLinear_23 + Conv_27, Tactic: 0xbb88763c3b0e94d4, 124 (Int8[1,64:32,58,58]) -> 137 (Int8[1,64:32,56,56])
Layer(CaskConvolution): conv4.3.weight_clone_0 + QuantizeLinear_227 + Conv_231, Tactic: 0xd3f592fae61c7986, 332 (Int8[1,64:32,22,22]) -> 345 (Int8[1,64:32,20,20])
Layer(CaskConvolution): conv4.3.weight_clone_1 + QuantizeLinear_376 + Conv_380, Tactic: 0xd3f592fae61c7986, 481 (Int8[1,64:32,22,22]) -> 494 (Int8[1,64:32,20,20])
Layer(CaskConvolution): conv4.3.weight_clone_2 + QuantizeLinear_526 + Conv_530, Tactic: 0xd3f592fae61c7986, 631 (Int8[1,64:32,22,22]) -> 644 (Int8[1,64:32,20,20])
Layer(CaskConvolution): conv4.3.weight_clone_3 + QuantizeLinear_676 + Conv_680, Tactic: 0xd3f592fae61c7986, 781 (Int8[1,64:32,22,22]) -> 794 (Int8[1,64:32,20,20])
Layer(CaskConvolution): conv4.3.weight_clone_4 + QuantizeLinear_826 + Conv_830, Tactic: 0xd3f592fae61c7986, 931 (Int8[1,64:32,22,22]) -> 944 (Int8[1,64:32,20,20])
Layer(CaskConvolution): conv4.3.weight_clone_5 + QuantizeLinear_976 + Conv_980, Tactic: 0xd3f592fae61c7986, 1081 (Int8[1,64:32,22,22]) -> 1094 (Int8[1,64:32,20,20])
Layer(CaskConvolution): conv4.3.weight_clone_6 + QuantizeLinear_1126 + Conv_1130, Tactic: 0xd3f592fae61c7986, 1231 (Int8[1,64:32,22,22]) -> 1244 (Int8[1,64:32,20,20])
Layer(CaskConvolution): conv4.3.weight_clone_7 + QuantizeLinear_1276 + Conv_1280, Tactic: 0xd3f592fae61c7986, 1381 (Int8[1,64:32,22,22]) -> 1394 (Int8[1,64:32,20,20])
Layer(CaskConvolution): conv4.3.weight_clone_8 + QuantizeLinear_1426 + Conv_1430, Tactic: 0xd3f592fae61c7986, 1531 (Int8[1,64:32,22,22]) -> 1544 (Int8[1,64:32,20,20])
Layer(CaskConvolution): conv4.3.weight_clone_9 + QuantizeLinear_1576 + Conv_1580, Tactic: 0xd3f592fae61c7986, 1681 (Int8[1,64:32,22,22]) -> 1694 (Int8[1,64:32,20,20])
Layer(CaskConvolution): conv4.3.weight_clone_10 + QuantizeLinear_1726 + Conv_1730, Tactic: 0xd3f592fae61c7986, 1831 (Int8[1,64:32,22,22]) -> 1844 (Int8[1,64:32,20,20])
Layer(CaskConvolution): conv4.3.weight_clone_11 + QuantizeLinear_1876 + Conv_1880, Tactic: 0xd3f592fae61c7986, 1981 (Int8[1,64:32,22,22]) -> 1994 (Int8[1,64:32,20,20])
Layer(CaskConvolution): conv4.3.weight_clone_12 + QuantizeLinear_2026 + Conv_2030, Tactic: 0xd3f592fae61c7986, 2131 (Int8[1,64:32,22,22]) -> 2144 (Int8[1,64:32,20,20])
Layer(CaskConvolution): conv4.3.weight_clone_13 + QuantizeLinear_2176 + Conv_2180, Tactic: 0xd3f592fae61c7986, 2281 (Int8[1,64:32,22,22]) -> 2294 (Int8[1,64:32,20,20])
Layer(CaskConvolution): conv4.3.weight_clone_14 + QuantizeLinear_2326 + Conv_2330, Tactic: 0xd3f592fae61c7986, 2431 (Int8[1,64:32,22,22]) -> 2444 (Int8[1,64:32,20,20])
Layer(CaskConvolution): conv4.3.weight_clone_15 + QuantizeLinear_2476 + Conv_2480, Tactic: 0xd3f592fae61c7986, 2581 (Int8[1,64:32,22,22]) -> 2594 (Int8[1,64:32,20,20])
Layer(CaskPooling): MaxPool_30, Tactic: 0x94215b398b8eb3ba, 137 (Int8[1,64:32,56,56]) -> 140 (Int8[1,64:32,28,28])
Layer(CaskPooling): MaxPool_234, Tactic: 0x94215b398b8eb3ba, 345 (Int8[1,64:32,20,20]) -> 348 (Int8[1,64:32,10,10])
Layer(CaskPooling): MaxPool_383, Tactic: 0x94215b398b8eb3ba, 494 (Int8[1,64:32,20,20]) -> 497 (Int8[1,64:32,10,10])
Layer(CaskPooling): MaxPool_533, Tactic: 0x94215b398b8eb3ba, 644 (Int8[1,64:32,20,20]) -> 647 (Int8[1,64:32,10,10])
Layer(CaskPooling): MaxPool_683, Tactic: 0x94215b398b8eb3ba, 794 (Int8[1,64:32,20,20]) -> 797 (Int8[1,64:32,10,10])
Layer(CaskPooling): MaxPool_833, Tactic: 0x94215b398b8eb3ba, 944 (Int8[1,64:32,20,20]) -> 947 (Int8[1,64:32,10,10])
Layer(CaskPooling): MaxPool_983, Tactic: 0x94215b398b8eb3ba, 1094 (Int8[1,64:32,20,20]) -> 1097 (Int8[1,64:32,10,10])
Layer(CaskPooling): MaxPool_1133, Tactic: 0x94215b398b8eb3ba, 1244 (Int8[1,64:32,20,20]) -> 1247 (Int8[1,64:32,10,10])
Layer(CaskPooling): MaxPool_1283, Tactic: 0x94215b398b8eb3ba, 1394 (Int8[1,64:32,20,20]) -> 1397 (Int8[1,64:32,10,10])
Layer(CaskPooling): MaxPool_1433, Tactic: 0x94215b398b8eb3ba, 1544 (Int8[1,64:32,20,20]) -> 1547 (Int8[1,64:32,10,10])
Layer(CaskPooling): MaxPool_1583, Tactic: 0x94215b398b8eb3ba, 1694 (Int8[1,64:32,20,20]) -> 1697 (Int8[1,64:32,10,10])
Layer(CaskPooling): MaxPool_1733, Tactic: 0x94215b398b8eb3ba, 1844 (Int8[1,64:32,20,20]) -> 1847 (Int8[1,64:32,10,10])
Layer(CaskPooling): MaxPool_1883, Tactic: 0x94215b398b8eb3ba, 1994 (Int8[1,64:32,20,20]) -> 1997 (Int8[1,64:32,10,10])
Layer(CaskPooling): MaxPool_2033, Tactic: 0x94215b398b8eb3ba, 2144 (Int8[1,64:32,20,20]) -> 2147 (Int8[1,64:32,10,10])
Layer(CaskPooling): MaxPool_2183, Tactic: 0x94215b398b8eb3ba, 2294 (Int8[1,64:32,20,20]) -> 2297 (Int8[1,64:32,10,10])
Layer(CaskPooling): MaxPool_2333, Tactic: 0x94215b398b8eb3ba, 2444 (Int8[1,64:32,20,20]) -> 2447 (Int8[1,64:32,10,10])
Layer(CaskPooling): MaxPool_2483, Tactic: 0x94215b398b8eb3ba, 2594 (Int8[1,64:32,20,20]) -> 2597 (Int8[1,64:32,10,10])
Layer(CaskConvolution): conv2.0.weight + QuantizeLinear_39 + Conv_43, Tactic: 0x4749124f62d8bd23, 140 (Int8[1,64:32,28,28]) -> 155 (Int8[1,128:32,26,26])
Layer(CaskConvolution): conv5.0.weight_clone_0 + QuantizeLinear_243 + Conv_247, Tactic: 0x4749124f62d8bd23, 348 (Int8[1,64:32,10,10]) -> 363 (Int8[1,128:32,8,8])
Layer(CaskConvolution): conv5.0.weight_clone_1 + QuantizeLinear_392 + Conv_396, Tactic: 0x4749124f62d8bd23, 497 (Int8[1,64:32,10,10]) -> 512 (Int8[1,128:32,8,8])
Layer(CaskConvolution): conv5.0.weight_clone_2 + QuantizeLinear_542 + Conv_546, Tactic: 0x4749124f62d8bd23, 647 (Int8[1,64:32,10,10]) -> 662 (Int8[1,128:32,8,8])
Layer(CaskConvolution): conv5.0.weight_clone_3 + QuantizeLinear_692 + Conv_696, Tactic: 0x4749124f62d8bd23, 797 (Int8[1,64:32,10,10]) -> 812 (Int8[1,128:32,8,8])
Layer(CaskConvolution): conv5.0.weight_clone_4 + QuantizeLinear_842 + Conv_846, Tactic: 0x4749124f62d8bd23, 947 (Int8[1,64:32,10,10]) -> 962 (Int8[1,128:32,8,8])
Layer(CaskConvolution): conv5.0.weight_clone_5 + QuantizeLinear_992 + Conv_996, Tactic: 0x4749124f62d8bd23, 1097 (Int8[1,64:32,10,10]) -> 1112 (Int8[1,128:32,8,8])
Layer(CaskConvolution): conv5.0.weight_clone_6 + QuantizeLinear_1142 + Conv_1146, Tactic: 0x4749124f62d8bd23, 1247 (Int8[1,64:32,10,10]) -> 1262 (Int8[1,128:32,8,8])
Layer(CaskConvolution): conv5.0.weight_clone_7 + QuantizeLinear_1292 + Conv_1296, Tactic: 0x4749124f62d8bd23, 1397 (Int8[1,64:32,10,10]) -> 1412 (Int8[1,128:32,8,8])
Layer(CaskConvolution): conv5.0.weight_clone_8 + QuantizeLinear_1442 + Conv_1446, Tactic: 0x4749124f62d8bd23, 1547 (Int8[1,64:32,10,10]) -> 1562 (Int8[1,128:32,8,8])
Layer(CaskConvolution): conv5.0.weight_clone_9 + QuantizeLinear_1592 + Conv_1596, Tactic: 0x4749124f62d8bd23, 1697 (Int8[1,64:32,10,10]) -> 1712 (Int8[1,128:32,8,8])
Layer(CaskConvolution): conv5.0.weight_clone_10 + QuantizeLinear_1742 + Conv_1746, Tactic: 0x4749124f62d8bd23, 1847 (Int8[1,64:32,10,10]) -> 1862 (Int8[1,128:32,8,8])
Layer(CaskConvolution): conv5.0.weight_clone_11 + QuantizeLinear_1892 + Conv_1896, Tactic: 0x4749124f62d8bd23, 1997 (Int8[1,64:32,10,10]) -> 2012 (Int8[1,128:32,8,8])
Layer(CaskConvolution): conv5.0.weight_clone_12 + QuantizeLinear_2042 + Conv_2046, Tactic: 0x4749124f62d8bd23, 2147 (Int8[1,64:32,10,10]) -> 2162 (Int8[1,128:32,8,8])
Layer(CaskConvolution): conv5.0.weight_clone_13 + QuantizeLinear_2192 + Conv_2196, Tactic: 0x4749124f62d8bd23, 2297 (Int8[1,64:32,10,10]) -> 2312 (Int8[1,128:32,8,8])
Layer(CaskConvolution): conv5.0.weight_clone_14 + QuantizeLinear_2342 + Conv_2346, Tactic: 0x4749124f62d8bd23, 2447 (Int8[1,64:32,10,10]) -> 2462 (Int8[1,128:32,8,8])
Layer(CaskConvolution): conv5.0.weight_clone_15 + QuantizeLinear_2492 + Conv_2496, Tactic: 0x4749124f62d8bd23, 2597 (Int8[1,64:32,10,10]) -> 2612 (Int8[1,128:32,8,8])
Layer(CaskConvolution): conv2.3.weight + QuantizeLinear_54 + Conv_58, Tactic: 0x85c1a5f7f239cf84, 155 (Int8[1,128:32,26,26]) -> 167 (Float[1,128,24,24])
Layer(CaskConvolution): conv5.3.weight_clone_0 + QuantizeLinear_258 + Conv_262, Tactic: 0x85c1a5f7f239cf84, 363 (Int8[1,128:32,8,8]) -> 375 (Float[1,128,6,6])
Layer(CaskConvolution): conv5.3.weight_clone_1 + QuantizeLinear_407 + Conv_411, Tactic: 0x85c1a5f7f239cf84, 512 (Int8[1,128:32,8,8]) -> 524 (Float[1,128,6,6])
Layer(CaskConvolution): conv5.3.weight_clone_2 + QuantizeLinear_557 + Conv_561, Tactic: 0x85c1a5f7f239cf84, 662 (Int8[1,128:32,8,8]) -> 674 (Float[1,128,6,6])
Layer(CaskConvolution): conv5.3.weight_clone_3 + QuantizeLinear_707 + Conv_711, Tactic: 0x85c1a5f7f239cf84, 812 (Int8[1,128:32,8,8]) -> 824 (Float[1,128,6,6])
Layer(CaskConvolution): conv5.3.weight_clone_4 + QuantizeLinear_857 + Conv_861, Tactic: 0x85c1a5f7f239cf84, 962 (Int8[1,128:32,8,8]) -> 974 (Float[1,128,6,6])
Layer(CaskConvolution): conv5.3.weight_clone_5 + QuantizeLinear_1007 + Conv_1011, Tactic: 0x85c1a5f7f239cf84, 1112 (Int8[1,128:32,8,8]) -> 1124 (Float[1,128,6,6])
Layer(CaskConvolution): conv5.3.weight_clone_6 + QuantizeLinear_1157 + Conv_1161, Tactic: 0x85c1a5f7f239cf84, 1262 (Int8[1,128:32,8,8]) -> 1274 (Float[1,128,6,6])
Layer(CaskConvolution): conv5.3.weight_clone_7 + QuantizeLinear_1307 + Conv_1311, Tactic: 0x85c1a5f7f239cf84, 1412 (Int8[1,128:32,8,8]) -> 1424 (Float[1,128,6,6])
Layer(CaskConvolution): conv5.3.weight_clone_8 + QuantizeLinear_1457 + Conv_1461, Tactic: 0x85c1a5f7f239cf84, 1562 (Int8[1,128:32,8,8]) -> 1574 (Float[1,128,6,6])
Layer(CaskConvolution): conv5.3.weight_clone_9 + QuantizeLinear_1607 + Conv_1611, Tactic: 0x85c1a5f7f239cf84, 1712 (Int8[1,128:32,8,8]) -> 1724 (Float[1,128,6,6])
Layer(CaskConvolution): conv5.3.weight_clone_10 + QuantizeLinear_1757 + Conv_1761, Tactic: 0x85c1a5f7f239cf84, 1862 (Int8[1,128:32,8,8]) -> 1874 (Float[1,128,6,6])
Layer(CaskConvolution): conv5.3.weight_clone_11 + QuantizeLinear_1907 + Conv_1911, Tactic: 0x85c1a5f7f239cf84, 2012 (Int8[1,128:32,8,8]) -> 2024 (Float[1,128,6,6])
Layer(CaskConvolution): conv5.3.weight_clone_12 + QuantizeLinear_2057 + Conv_2061, Tactic: 0x85c1a5f7f239cf84, 2162 (Int8[1,128:32,8,8]) -> 2174 (Float[1,128,6,6])
Layer(CaskConvolution): conv5.3.weight_clone_13 + QuantizeLinear_2207 + Conv_2211, Tactic: 0x85c1a5f7f239cf84, 2312 (Int8[1,128:32,8,8]) -> 2324 (Float[1,128,6,6])
Layer(CaskConvolution): conv5.3.weight_clone_14 + QuantizeLinear_2357 + Conv_2361, Tactic: 0x85c1a5f7f239cf84, 2462 (Int8[1,128:32,8,8]) -> 2474 (Float[1,128,6,6])
Layer(CaskConvolution): conv5.3.weight_clone_15 + QuantizeLinear_2507 + Conv_2511, Tactic: 0x85c1a5f7f239cf84, 2612 (Int8[1,128:32,8,8]) -> 2624 (Float[1,128,6,6])
Layer(CaskPooling): MaxPool_61, Tactic: 0x60eceb67eff69444, 167 (Float[1,128,24,24]) -> 168 (Float[1,128,12,12])
Layer(Reformat): QuantizeLinear_71, Tactic: 0x0000000000000000, 168 (Float[1,128,12,12]) -> 182 (Int8[1,128:32,12,12])
Layer(CaskPooling): MaxPool_265, Tactic: 0xf4b20242f8c135e7, 375 (Float[1,128,6,6]) -> 376 (Float[1,128,3,3])
Layer(CaskPooling): MaxPool_414, Tactic: 0xf4b20242f8c135e7, 524 (Float[1,128,6,6]) -> 525 (Float[1,128,3,3])
Layer(CaskPooling): MaxPool_564, Tactic: 0xf4b20242f8c135e7, 674 (Float[1,128,6,6]) -> 675 (Float[1,128,3,3])
Layer(CaskPooling): MaxPool_714, Tactic: 0xf4b20242f8c135e7, 824 (Float[1,128,6,6]) -> 825 (Float[1,128,3,3])
Layer(CaskPooling): MaxPool_864, Tactic: 0xf4b20242f8c135e7, 974 (Float[1,128,6,6]) -> 975 (Float[1,128,3,3])
Layer(CaskPooling): MaxPool_1014, Tactic: 0xf4b20242f8c135e7, 1124 (Float[1,128,6,6]) -> 1125 (Float[1,128,3,3])
Layer(CaskPooling): MaxPool_1164, Tactic: 0xf4b20242f8c135e7, 1274 (Float[1,128,6,6]) -> 1275 (Float[1,128,3,3])
Layer(CaskPooling): MaxPool_1314, Tactic: 0xf4b20242f8c135e7, 1424 (Float[1,128,6,6]) -> 1425 (Float[1,128,3,3])
Layer(CaskPooling): MaxPool_1464, Tactic: 0xf4b20242f8c135e7, 1574 (Float[1,128,6,6]) -> 1575 (Float[1,128,3,3])
Layer(CaskPooling): MaxPool_1614, Tactic: 0xf4b20242f8c135e7, 1724 (Float[1,128,6,6]) -> 1725 (Float[1,128,3,3])
Layer(CaskPooling): MaxPool_1764, Tactic: 0xf4b20242f8c135e7, 1874 (Float[1,128,6,6]) -> 1875 (Float[1,128,3,3])
Layer(CaskPooling): MaxPool_1914, Tactic: 0xf4b20242f8c135e7, 2024 (Float[1,128,6,6]) -> 2025 (Float[1,128,3,3])
Layer(CaskPooling): MaxPool_2064, Tactic: 0xf4b20242f8c135e7, 2174 (Float[1,128,6,6]) -> 2175 (Float[1,128,3,3])
Layer(CaskPooling): MaxPool_2214, Tactic: 0xf4b20242f8c135e7, 2324 (Float[1,128,6,6]) -> 2325 (Float[1,128,3,3])
Layer(CaskPooling): MaxPool_2364, Tactic: 0xf4b20242f8c135e7, 2474 (Float[1,128,6,6]) -> 2475 (Float[1,128,3,3])
Layer(CaskPooling): MaxPool_2514, Tactic: 0xf4b20242f8c135e7, 2624 (Float[1,128,6,6]) -> 2625 (Float[1,128,3,3])
Layer(Reduce): ReduceMean_266, Tactic: 0x0000000000000005, 376 (Float[1,128,3,3]) -> 377 (Float[1,1,3,3])
Layer(Reduce): ReduceMax_267, Tactic: 0x0000000000000005, 376 (Float[1,128,3,3]) -> 378 (Float[1,1,3,3])
Layer(Reduce): ReduceMean_415, Tactic: 0x0000000000000005, 525 (Float[1,128,3,3]) -> 526 (Float[1,1,3,3])
Layer(Reduce): ReduceMax_416, Tactic: 0x0000000000000005, 525 (Float[1,128,3,3]) -> 527 (Float[1,1,3,3])
Layer(Reduce): ReduceMean_565, Tactic: 0x0000000000000005, 675 (Float[1,128,3,3]) -> 676 (Float[1,1,3,3])
Layer(Reduce): ReduceMax_566, Tactic: 0x0000000000000005, 675 (Float[1,128,3,3]) -> 677 (Float[1,1,3,3])
Layer(Reduce): ReduceMean_715, Tactic: 0x0000000000000005, 825 (Float[1,128,3,3]) -> 826 (Float[1,1,3,3])
Layer(Reduce): ReduceMax_716, Tactic: 0x0000000000000005, 825 (Float[1,128,3,3]) -> 827 (Float[1,1,3,3])
Layer(Reduce): ReduceMean_865, Tactic: 0x0000000000000005, 975 (Float[1,128,3,3]) -> 976 (Float[1,1,3,3])
Layer(Reduce): ReduceMax_866, Tactic: 0x0000000000000005, 975 (Float[1,128,3,3]) -> 977 (Float[1,1,3,3])
Layer(Reduce): ReduceMean_1015, Tactic: 0x0000000000000005, 1125 (Float[1,128,3,3]) -> 1126 (Float[1,1,3,3])
Layer(Reduce): ReduceMax_1016, Tactic: 0x0000000000000005, 1125 (Float[1,128,3,3]) -> 1127 (Float[1,1,3,3])
Layer(Reduce): ReduceMean_1165, Tactic: 0x0000000000000005, 1275 (Float[1,128,3,3]) -> 1276 (Float[1,1,3,3])
Layer(Reduce): ReduceMax_1166, Tactic: 0x0000000000000005, 1275 (Float[1,128,3,3]) -> 1277 (Float[1,1,3,3])
Layer(Reduce): ReduceMean_1315, Tactic: 0x0000000000000005, 1425 (Float[1,128,3,3]) -> 1426 (Float[1,1,3,3])
Layer(Reduce): ReduceMax_1316, Tactic: 0x0000000000000005, 1425 (Float[1,128,3,3]) -> 1427 (Float[1,1,3,3])
Layer(Reduce): ReduceMean_1465, Tactic: 0x0000000000000005, 1575 (Float[1,128,3,3]) -> 1576 (Float[1,1,3,3])
Layer(Reduce): ReduceMax_1466, Tactic: 0x0000000000000005, 1575 (Float[1,128,3,3]) -> 1577 (Float[1,1,3,3])
Layer(Reduce): ReduceMean_1615, Tactic: 0x0000000000000005, 1725 (Float[1,128,3,3]) -> 1726 (Float[1,1,3,3])
Layer(Reduce): ReduceMax_1616, Tactic: 0x0000000000000005, 1725 (Float[1,128,3,3]) -> 1727 (Float[1,1,3,3])
Layer(Reduce): ReduceMean_1765, Tactic: 0x0000000000000005, 1875 (Float[1,128,3,3]) -> 1876 (Float[1,1,3,3])
Layer(Reduce): ReduceMax_1766, Tactic: 0x0000000000000005, 1875 (Float[1,128,3,3]) -> 1877 (Float[1,1,3,3])
Layer(Reduce): ReduceMean_1915, Tactic: 0x0000000000000005, 2025 (Float[1,128,3,3]) -> 2026 (Float[1,1,3,3])
Layer(Reduce): ReduceMax_1916, Tactic: 0x0000000000000005, 2025 (Float[1,128,3,3]) -> 2027 (Float[1,1,3,3])
Layer(Reduce): ReduceMean_2065, Tactic: 0x0000000000000005, 2175 (Float[1,128,3,3]) -> 2176 (Float[1,1,3,3])
Layer(Reduce): ReduceMax_2066, Tactic: 0x0000000000000005, 2175 (Float[1,128,3,3]) -> 2177 (Float[1,1,3,3])
Layer(Reduce): ReduceMean_2215, Tactic: 0x0000000000000005, 2325 (Float[1,128,3,3]) -> 2326 (Float[1,1,3,3])
Layer(Reduce): ReduceMax_2216, Tactic: 0x0000000000000005, 2325 (Float[1,128,3,3]) -> 2327 (Float[1,1,3,3])
Layer(Reduce): ReduceMean_2365, Tactic: 0x0000000000000005, 2475 (Float[1,128,3,3]) -> 2476 (Float[1,1,3,3])
Layer(Reduce): ReduceMax_2366, Tactic: 0x0000000000000005, 2475 (Float[1,128,3,3]) -> 2477 (Float[1,1,3,3])
Layer(Reduce): ReduceMean_2515, Tactic: 0x0000000000000005, 2625 (Float[1,128,3,3]) -> 2626 (Float[1,1,3,3])
Layer(Reduce): ReduceMax_2516, Tactic: 0x0000000000000005, 2625 (Float[1,128,3,3]) -> 2627 (Float[1,1,3,3])
Layer(CaskConvolution): conv3.0.weight + QuantizeLinear_77 + Conv_81, Tactic: 0x4749124f62d8bd23, 182 (Int8[1,128:32,12,12]) -> 197 (Int8[1,256:32,10,10])
Layer(Reformat): QuantizeLinear_271_clone_1, Tactic: 0x00000000000003e8, 378 (Float[1,1,3,3]) -> 382 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_271_clone_0, Tactic: 0x00000000000003e8, 377 (Float[1,1,3,3]) -> Concat_268_377_clone_0 (Int8[1,1,3,3])
Layer(Reformat): Concat_268_377_clone_0 copy, Tactic: 0x00000000000003e8, Concat_268_377_clone_0 (Int8[1,1,3,3]) -> 382 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_420_clone_1, Tactic: 0x00000000000003e8, 527 (Float[1,1,3,3]) -> 531 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_420_clone_0, Tactic: 0x0000000000000000, 526 (Float[1,1,3,3]) -> Concat_417_526_clone_0 (Int8[1,1:4,3,3])
Layer(Reformat): Concat_417_526_clone_0 copy, Tactic: 0x00000000000003e8, Concat_417_526_clone_0 (Int8[1,1:4,3,3]) -> 531 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_570_clone_1, Tactic: 0x00000000000003e8, 677 (Float[1,1,3,3]) -> 681 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_570_clone_0, Tactic: 0x00000000000003e8, 676 (Float[1,1,3,3]) -> Concat_567_676_clone_0 (Int8[1,1:32,3,3])
Layer(Reformat): Concat_567_676_clone_0 copy, Tactic: 0x0000000000000000, Concat_567_676_clone_0 (Int8[1,1:32,3,3]) -> 681 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_720_clone_1, Tactic: 0x0000000000000000, 827 (Float[1,1,3,3]) -> 831 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_720_clone_0, Tactic: 0x0000000000000000, 826 (Float[1,1,3,3]) -> Concat_717_826_clone_0 (Int8[1,1:4,3,3])
Layer(Reformat): Concat_717_826_clone_0 copy, Tactic: 0x00000000000003e8, Concat_717_826_clone_0 (Int8[1,1:4,3,3]) -> 831 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_870_clone_1, Tactic: 0x00000000000003e8, 977 (Float[1,1,3,3]) -> 981 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_870_clone_0, Tactic: 0x00000000000003e8, 976 (Float[1,1,3,3]) -> Concat_867_976_clone_0 (Int8[1:32,1,3,3])
Layer(Reformat): Concat_867_976_clone_0 copy, Tactic: 0x00000000000003e8, Concat_867_976_clone_0 (Int8[1:32,1,3,3]) -> 981 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_1020_clone_1, Tactic: 0x0000000000000000, 1127 (Float[1,1,3,3]) -> 1131 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_1020_clone_0, Tactic: 0x00000000000003e8, 1126 (Float[1,1,3,3]) -> Concat_1017_1126_clone_0 (Int8[1,1,3,3])
Layer(Reformat): Concat_1017_1126_clone_0 copy, Tactic: 0x00000000000003e8, Concat_1017_1126_clone_0 (Int8[1,1,3,3]) -> 1131 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_1170_clone_1, Tactic: 0x00000000000003e8, 1277 (Float[1,1,3,3]) -> 1281 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_1170_clone_0, Tactic: 0x00000000000003e8, 1276 (Float[1,1,3,3]) -> Concat_1167_1276_clone_0 (Int8[1,1,3,3])
Layer(Reformat): Concat_1167_1276_clone_0 copy, Tactic: 0x00000000000003e8, Concat_1167_1276_clone_0 (Int8[1,1,3,3]) -> 1281 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_1320_clone_1, Tactic: 0x0000000000000000, 1427 (Float[1,1,3,3]) -> 1431 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_1320_clone_0, Tactic: 0x00000000000003e8, 1426 (Float[1,1,3,3]) -> Concat_1317_1426_clone_0 (Int8[1,1:4,3,3])
Layer(Reformat): Concat_1317_1426_clone_0 copy, Tactic: 0x00000000000003e8, Concat_1317_1426_clone_0 (Int8[1,1:4,3,3]) -> 1431 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_1470_clone_1, Tactic: 0x00000000000003e8, 1577 (Float[1,1,3,3]) -> 1581 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_1470_clone_0, Tactic: 0x0000000000000000, 1576 (Float[1,1,3,3]) -> Concat_1467_1576_clone_0 (Int8[1,1:4,3,3])
Layer(Reformat): Concat_1467_1576_clone_0 copy, Tactic: 0x00000000000003e8, Concat_1467_1576_clone_0 (Int8[1,1:4,3,3]) -> 1581 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_1620_clone_1, Tactic: 0x00000000000003e8, 1727 (Float[1,1,3,3]) -> 1731 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_1620_clone_0, Tactic: 0x0000000000000000, 1726 (Float[1,1,3,3]) -> Concat_1617_1726_clone_0 (Int8[1,1:4,3,3])
Layer(Reformat): Concat_1617_1726_clone_0 copy, Tactic: 0x00000000000003e8, Concat_1617_1726_clone_0 (Int8[1,1:4,3,3]) -> 1731 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_1770_clone_1, Tactic: 0x00000000000003e8, 1877 (Float[1,1,3,3]) -> 1881 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_1770_clone_0, Tactic: 0x00000000000003e8, 1876 (Float[1,1,3,3]) -> Concat_1767_1876_clone_0 (Int8[1,1,3,3])
Layer(Reformat): Concat_1767_1876_clone_0 copy, Tactic: 0x0000000000000000, Concat_1767_1876_clone_0 (Int8[1,1,3,3]) -> 1881 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_1920_clone_1, Tactic: 0x0000000000000000, 2027 (Float[1,1,3,3]) -> 2031 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_1920_clone_0, Tactic: 0x0000000000000000, 2026 (Float[1,1,3,3]) -> Concat_1917_2026_clone_0 (Int8[1,1:4,3,3])
Layer(Reformat): Concat_1917_2026_clone_0 copy, Tactic: 0x00000000000003e8, Concat_1917_2026_clone_0 (Int8[1,1:4,3,3]) -> 2031 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_2070_clone_1, Tactic: 0x0000000000000000, 2177 (Float[1,1,3,3]) -> 2181 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_2070_clone_0, Tactic: 0x00000000000003e8, 2176 (Float[1,1,3,3]) -> Concat_2067_2176_clone_0 (Int8[1,1,3,3])
Layer(Reformat): Concat_2067_2176_clone_0 copy, Tactic: 0x00000000000003e8, Concat_2067_2176_clone_0 (Int8[1,1,3,3]) -> 2181 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_2220_clone_1, Tactic: 0x00000000000003e8, 2327 (Float[1,1,3,3]) -> 2331 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_2220_clone_0, Tactic: 0x00000000000003e8, 2326 (Float[1,1,3,3]) -> Concat_2217_2326_clone_0 (Int8[1,1,3,3])
Layer(Reformat): Concat_2217_2326_clone_0 copy, Tactic: 0x00000000000003e8, Concat_2217_2326_clone_0 (Int8[1,1,3,3]) -> 2331 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_2370_clone_1, Tactic: 0x00000000000003e8, 2477 (Float[1,1,3,3]) -> 2481 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_2370_clone_0, Tactic: 0x0000000000000000, 2476 (Float[1,1,3,3]) -> Concat_2367_2476_clone_0 (Int8[1,1:4,3,3])
Layer(Reformat): Concat_2367_2476_clone_0 copy, Tactic: 0x00000000000003e8, Concat_2367_2476_clone_0 (Int8[1,1:4,3,3]) -> 2481 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_2520_clone_1, Tactic: 0x00000000000003e8, 2627 (Float[1,1,3,3]) -> 2631 (Int8[1,1:4,3,3])
Layer(Reformat): QuantizeLinear_2520_clone_0, Tactic: 0x00000000000003e8, 2626 (Float[1,1,3,3]) -> Concat_2517_2626_clone_0 (Int8[1,1,3,3])
Layer(Reformat): Concat_2517_2626_clone_0 copy, Tactic: 0x00000000000003e8, Concat_2517_2626_clone_0 (Int8[1,1,3,3]) -> 2631 (Int8[1,1:4,3,3])
Layer(Resize): Resize_68, Tactic: 0x0000000000000000, 168 (Float[1,128,12,12]) -> 179 (Float[1,128,4,4])
Layer(CaskConvolution): patchattention_spatial.conv1.weight_clone_0 + QuantizeLinear_277 + Conv_281, Tactic: 0x1b0534177b414e71, 382 (Int8[1,2:4,3,3]) -> 392 (Float[1,1,3,3])
Layer(CaskConvolution): patchattention_spatial.conv1.weight_clone_1 + QuantizeLinear_426 + Conv_430, Tactic: 0x1b0534177b414e71, 531 (Int8[1,2:4,3,3]) -> 541 (Float[1,1,3,3])
Layer(CaskConvolution): patchattention_spatial.conv1.weight_clone_2 + QuantizeLinear_576 + Conv_580, Tactic: 0x1b0534177b414e71, 681 (Int8[1,2:4,3,3]) -> 691 (Float[1,1,3,3])
Layer(CaskConvolution): patchattention_spatial.conv1.weight_clone_3 + QuantizeLinear_726 + Conv_730, Tactic: 0x1b0534177b414e71, 831 (Int8[1,2:4,3,3]) -> 841 (Float[1,1,3,3])
Layer(CaskConvolution): patchattention_spatial.conv1.weight_clone_4 + QuantizeLinear_876 + Conv_880, Tactic: 0x1b0534177b414e71, 981 (Int8[1,2:4,3,3]) -> 991 (Float[1,1,3,3])
Layer(CaskConvolution): patchattention_spatial.conv1.weight_clone_5 + QuantizeLinear_1026 + Conv_1030, Tactic: 0x1b0534177b414e71, 1131 (Int8[1,2:4,3,3]) -> 1141 (Float[1,1,3,3])
Layer(CaskConvolution): patchattention_spatial.conv1.weight_clone_6 + QuantizeLinear_1176 + Conv_1180, Tactic: 0x1b0534177b414e71, 1281 (Int8[1,2:4,3,3]) -> 1291 (Float[1,1,3,3])
Layer(CaskConvolution): patchattention_spatial.conv1.weight_clone_7 + QuantizeLinear_1326 + Conv_1330, Tactic: 0x1b0534177b414e71, 1431 (Int8[1,2:4,3,3]) -> 1441 (Float[1,1,3,3])
Layer(CaskConvolution): patchattention_spatial.conv1.weight_clone_8 + QuantizeLinear_1476 + Conv_1480, Tactic: 0x1b0534177b414e71, 1581 (Int8[1,2:4,3,3]) -> 1591 (Float[1,1,3,3])
Layer(CaskConvolution): patchattention_spatial.conv1.weight_clone_9 + QuantizeLinear_1626 + Conv_1630, Tactic: 0x1b0534177b414e71, 1731 (Int8[1,2:4,3,3]) -> 1741 (Float[1,1,3,3])
Layer(CaskConvolution): patchattention_spatial.conv1.weight_clone_10 + QuantizeLinear_1776 + Conv_1780, Tactic: 0x1b0534177b414e71, 1881 (Int8[1,2:4,3,3]) -> 1891 (Float[1,1,3,3])
Layer(CaskConvolution): patchattention_spatial.conv1.weight_clone_11 + QuantizeLinear_1926 + Conv_1930, Tactic: 0x1b0534177b414e71, 2031 (Int8[1,2:4,3,3]) -> 2041 (Float[1,1,3,3])
Layer(CaskConvolution): patchattention_spatial.conv1.weight_clone_12 + QuantizeLinear_2076 + Conv_2080, Tactic: 0x1b0534177b414e71, 2181 (Int8[1,2:4,3,3]) -> 2191 (Float[1,1,3,3])
Layer(CaskConvolution): patchattention_spatial.conv1.weight_clone_13 + QuantizeLinear_2226 + Conv_2230, Tactic: 0x1b0534177b414e71, 2331 (Int8[1,2:4,3,3]) -> 2341 (Float[1,1,3,3])
Layer(CaskConvolution): patchattention_spatial.conv1.weight_clone_14 + QuantizeLinear_2376 + Conv_2380, Tactic: 0x1b0534177b414e71, 2481 (Int8[1,2:4,3,3]) -> 2491 (Float[1,1,3,3])
Layer(CaskConvolution): patchattention_spatial.conv1.weight_clone_15 + QuantizeLinear_2526 + Conv_2530, Tactic: 0x1b0534177b414e71, 2631 (Int8[1,2:4,3,3]) -> 2641 (Float[1,1,3,3])
Layer(CaskConvolution): conv3.3.weight + QuantizeLinear_92 + Conv_96, Tactic: 0x85c1a5f7f239cf84, 197 (Int8[1,256:32,10,10]) -> 209 (Float[1,256,8,8])
Layer(PointWiseV2): PWN(Sigmoid_282, Mul_283), Tactic: 0x0000000000000004, 392 (Float[1,1,3,3]), 376 (Float[1,128,3,3]) -> 394 (Float[1,128,3,3])
Layer(PointWiseV2): PWN(Sigmoid_431, Mul_432), Tactic: 0x0000000000000004, 541 (Float[1,1,3,3]), 525 (Float[1,128,3,3]) -> 543 (Float[1,128,3,3])
Layer(PointWiseV2): PWN(Sigmoid_581, Mul_582), Tactic: 0x0000000000000004, 691 (Float[1,1,3,3]), 675 (Float[1,128,3,3]) -> 693 (Float[1,128,3,3])
Layer(PointWiseV2): PWN(Sigmoid_731, Mul_732), Tactic: 0x0000000000000004, 841 (Float[1,1,3,3]), 825 (Float[1,128,3,3]) -> 843 (Float[1,128,3,3])
Layer(PointWiseV2): PWN(Sigmoid_881, Mul_882), Tactic: 0x0000000000000004, 991 (Float[1,1,3,3]), 975 (Float[1,128,3,3]) -> 993 (Float[1,128,3,3])
Layer(PointWiseV2): PWN(Sigmoid_1031, Mul_1032), Tactic: 0x0000000000000004, 1141 (Float[1,1,3,3]), 1125 (Float[1,128,3,3]) -> 1143 (Float[1,128,3,3])
Layer(PointWiseV2): PWN(Sigmoid_1181, Mul_1182), Tactic: 0x0000000000000004, 1291 (Float[1,1,3,3]), 1275 (Float[1,128,3,3]) -> 1293 (Float[1,128,3,3])
Layer(PointWiseV2): PWN(Sigmoid_1331, Mul_1332), Tactic: 0x0000000000000004, 1441 (Float[1,1,3,3]), 1425 (Float[1,128,3,3]) -> 1443 (Float[1,128,3,3])
Layer(PointWiseV2): PWN(Sigmoid_1481, Mul_1482), Tactic: 0x0000000000000004, 1591 (Float[1,1,3,3]), 1575 (Float[1,128,3,3]) -> 1593 (Float[1,128,3,3])
Layer(PointWiseV2): PWN(Sigmoid_1631, Mul_1632), Tactic: 0x0000000000000004, 1741 (Float[1,1,3,3]), 1725 (Float[1,128,3,3]) -> 1743 (Float[1,128,3,3])
Layer(PointWiseV2): PWN(Sigmoid_1781, Mul_1782), Tactic: 0x0000000000000004, 1891 (Float[1,1,3,3]), 1875 (Float[1,128,3,3]) -> 1893 (Float[1,128,3,3])
Layer(PointWiseV2): PWN(Sigmoid_1931, Mul_1932), Tactic: 0x0000000000000004, 2041 (Float[1,1,3,3]), 2025 (Float[1,128,3,3]) -> 2043 (Float[1,128,3,3])
Layer(PointWiseV2): PWN(Sigmoid_2081, Mul_2082), Tactic: 0x0000000000000004, 2191 (Float[1,1,3,3]), 2175 (Float[1,128,3,3]) -> 2193 (Float[1,128,3,3])
Layer(PointWiseV2): PWN(Sigmoid_2231, Mul_2232), Tactic: 0x0000000000000004, 2341 (Float[1,1,3,3]), 2325 (Float[1,128,3,3]) -> 2343 (Float[1,128,3,3])
Layer(PointWiseV2): PWN(Sigmoid_2381, Mul_2382), Tactic: 0x0000000000000004, 2491 (Float[1,1,3,3]), 2475 (Float[1,128,3,3]) -> 2493 (Float[1,128,3,3])
Layer(PointWiseV2): PWN(Sigmoid_2531, Mul_2532), Tactic: 0x0000000000000004, 2641 (Float[1,1,3,3]), 2625 (Float[1,128,3,3]) -> 2643 (Float[1,128,3,3])
Layer(CaskPooling): GlobalAveragePool_284, Tactic: 0x8819e020387dbf3c, 394 (Float[1,128,3,3]) -> 395 (Float[1,128,1,1])
Layer(Reformat): QuantizeLinear_315, Tactic: 0x0000000000000000, 394 (Float[1,128,3,3]) -> 423 (Int8[1,128:4,3,3])
Layer(CaskPooling): GlobalAveragePool_433, Tactic: 0x8819e020387dbf3c, 543 (Float[1,128,3,3]) -> 544 (Float[1,128,1,1])
Layer(Reformat): QuantizeLinear_464, Tactic: 0x0000000000000000, 543 (Float[1,128,3,3]) -> 572 (Int8[1,128:4,3,3])
Layer(CaskPooling): GlobalAveragePool_583, Tactic: 0x8819e020387dbf3c, 693 (Float[1,128,3,3]) -> 694 (Float[1,128,1,1])
Layer(Reformat): QuantizeLinear_614, Tactic: 0x00000000000003e8, 693 (Float[1,128,3,3]) -> 722 (Int8[1,128:4,3,3])
Layer(CaskPooling): GlobalAveragePool_733, Tactic: 0x8819e020387dbf3c, 843 (Float[1,128,3,3]) -> 844 (Float[1,128,1,1])
Layer(Reformat): QuantizeLinear_764, Tactic: 0x0000000000000000, 843 (Float[1,128,3,3]) -> 872 (Int8[1,128:4,3,3])
Layer(CaskPooling): GlobalAveragePool_883, Tactic: 0x8819e020387dbf3c, 993 (Float[1,128,3,3]) -> 994 (Float[1,128,1,1])
Layer(Reformat): QuantizeLinear_914, Tactic: 0x00000000000003e8, 993 (Float[1,128,3,3]) -> 1022 (Int8[1,128:4,3,3])
Layer(CaskPooling): GlobalAveragePool_1033, Tactic: 0x8819e020387dbf3c, 1143 (Float[1,128,3,3]) -> 1144 (Float[1,128,1,1])
Layer(Reformat): QuantizeLinear_1064, Tactic: 0x0000000000000000, 1143 (Float[1,128,3,3]) -> 1172 (Int8[1,128:4,3,3])
Layer(CaskPooling): GlobalAveragePool_1183, Tactic: 0x8819e020387dbf3c, 1293 (Float[1,128,3,3]) -> 1294 (Float[1,128,1,1])
Layer(Reformat): QuantizeLinear_1214, Tactic: 0x00000000000003e8, 1293 (Float[1,128,3,3]) -> 1322 (Int8[1,128:4,3,3])
Layer(CaskPooling): GlobalAveragePool_1333, Tactic: 0x8819e020387dbf3c, 1443 (Float[1,128,3,3]) -> 1444 (Float[1,128,1,1])
Layer(Reformat): QuantizeLinear_1364, Tactic: 0x0000000000000000, 1443 (Float[1,128,3,3]) -> 1472 (Int8[1,128:4,3,3])
Layer(CaskPooling): GlobalAveragePool_1483, Tactic: 0x8819e020387dbf3c, 1593 (Float[1,128,3,3]) -> 1594 (Float[1,128,1,1])
Layer(Reformat): QuantizeLinear_1514, Tactic: 0x0000000000000000, 1593 (Float[1,128,3,3]) -> 1622 (Int8[1,128:4,3,3])
Layer(CaskPooling): GlobalAveragePool_1633, Tactic: 0x8819e020387dbf3c, 1743 (Float[1,128,3,3]) -> 1744 (Float[1,128,1,1])
Layer(Reformat): QuantizeLinear_1664, Tactic: 0x0000000000000000, 1743 (Float[1,128,3,3]) -> 1772 (Int8[1,128:4,3,3])
Layer(CaskPooling): GlobalAveragePool_1783, Tactic: 0x8819e020387dbf3c, 1893 (Float[1,128,3,3]) -> 1894 (Float[1,128,1,1])
Layer(Reformat): QuantizeLinear_1814, Tactic: 0x00000000000003e8, 1893 (Float[1,128,3,3]) -> 1922 (Int8[1,128:4,3,3])
Layer(CaskPooling): GlobalAveragePool_1933, Tactic: 0x8819e020387dbf3c, 2043 (Float[1,128,3,3]) -> 2044 (Float[1,128,1,1])
Layer(Reformat): QuantizeLinear_1964, Tactic: 0x0000000000000000, 2043 (Float[1,128,3,3]) -> 2072 (Int8[1,128:32,3,3])
Layer(CaskPooling): GlobalAveragePool_2083, Tactic: 0x8819e020387dbf3c, 2193 (Float[1,128,3,3]) -> 2194 (Float[1,128,1,1])
Layer(Reformat): QuantizeLinear_2114, Tactic: 0x0000000000000000, 2193 (Float[1,128,3,3]) -> 2222 (Int8[1,128:4,3,3])
Layer(CaskPooling): GlobalAveragePool_2233, Tactic: 0x8819e020387dbf3c, 2343 (Float[1,128,3,3]) -> 2344 (Float[1,128,1,1])
Layer(Reformat): QuantizeLinear_2264, Tactic: 0x0000000000000000, 2343 (Float[1,128,3,3]) -> 2372 (Int8[1,128:4,3,3])
Layer(CaskPooling): GlobalAveragePool_2383, Tactic: 0x8819e020387dbf3c, 2493 (Float[1,128,3,3]) -> 2494 (Float[1,128,1,1])
Layer(Reformat): QuantizeLinear_2414, Tactic: 0x0000000000000000, 2493 (Float[1,128,3,3]) -> 2522 (Int8[1,128:4,3,3])
Layer(CaskPooling): GlobalAveragePool_2533, Tactic: 0x8819e020387dbf3c, 2643 (Float[1,128,3,3]) -> 2644 (Float[1,128,1,1])
Layer(Reformat): QuantizeLinear_2564, Tactic: 0x0000000000000000, 2643 (Float[1,128,3,3]) -> 2672 (Int8[1,128:4,3,3])
Layer(Reformat): QuantizeLinear_287, Tactic: 0x00000000000003e8, 395 (Float[1,128,1,1]) -> 398 (Int8[1,128,1,1])
Layer(CaskPooling): MaxPool_312, Tactic: 0x1f6c40e3e09ec730, 423 (Int8[1,128:4,3,3]) -> 426 (Int8[1,128:4,1,1])
Layer(Reformat): QuantizeLinear_436, Tactic: 0x00000000000003e8, 544 (Float[1,128,1,1]) -> 547 (Int8[1,128,1,1])
Layer(CaskPooling): MaxPool_461, Tactic: 0x1f6c40e3e09ec730, 572 (Int8[1,128:4,3,3]) -> 575 (Int8[1,128:4,1,1])
Layer(Reformat): QuantizeLinear_586, Tactic: 0x00000000000003e8, 694 (Float[1,128,1,1]) -> 697 (Int8[1,128,1,1])
Layer(CaskPooling): MaxPool_611, Tactic: 0x1f6c40e3e09ec730, 722 (Int8[1,128:4,3,3]) -> 725 (Int8[1,128:4,1,1])
Layer(Reformat): QuantizeLinear_736, Tactic: 0x00000000000003e8, 844 (Float[1,128,1,1]) -> 847 (Int8[1,128,1,1])
Layer(CaskPooling): MaxPool_761, Tactic: 0x1f6c40e3e09ec730, 872 (Int8[1,128:4,3,3]) -> 875 (Int8[1,128:4,1,1])
Layer(Reformat): QuantizeLinear_886, Tactic: 0x00000000000003e8, 994 (Float[1,128,1,1]) -> 997 (Int8[1,128,1,1])
Layer(CaskPooling): MaxPool_911, Tactic: 0x1f6c40e3e09ec730, 1022 (Int8[1,128:4,3,3]) -> 1025 (Int8[1,128:4,1,1])
Layer(Reformat): QuantizeLinear_1036, Tactic: 0x00000000000003e8, 1144 (Float[1,128,1,1]) -> 1147 (Int8[1,128,1,1])
Layer(CaskPooling): MaxPool_1061, Tactic: 0x1f6c40e3e09ec730, 1172 (Int8[1,128:4,3,3]) -> 1175 (Int8[1,128:4,1,1])
Layer(Reformat): QuantizeLinear_1186, Tactic: 0x00000000000003e8, 1294 (Float[1,128,1,1]) -> 1297 (Int8[1,128,1,1])
Layer(CaskPooling): MaxPool_1211, Tactic: 0x1f6c40e3e09ec730, 1322 (Int8[1,128:4,3,3]) -> 1325 (Int8[1,128:4,1,1])
Layer(Reformat): QuantizeLinear_1336, Tactic: 0x00000000000003e8, 1444 (Float[1,128,1,1]) -> 1447 (Int8[1,128,1,1])
Layer(CaskPooling): MaxPool_1361, Tactic: 0x1f6c40e3e09ec730, 1472 (Int8[1,128:4,3,3]) -> 1475 (Int8[1,128:4,1,1])
Layer(Reformat): QuantizeLinear_1486, Tactic: 0x00000000000003e8, 1594 (Float[1,128,1,1]) -> 1597 (Int8[1,128,1,1])
Layer(CaskPooling): MaxPool_1511, Tactic: 0x1f6c40e3e09ec730, 1622 (Int8[1,128:4,3,3]) -> 1625 (Int8[1,128:4,1,1])
Layer(Reformat): QuantizeLinear_1636, Tactic: 0x00000000000003e8, 1744 (Float[1,128,1,1]) -> 1747 (Int8[1,128,1,1])
Layer(CaskPooling): MaxPool_1661, Tactic: 0x1f6c40e3e09ec730, 1772 (Int8[1,128:4,3,3]) -> 1775 (Int8[1,128:4,1,1])
Layer(Reformat): QuantizeLinear_1786, Tactic: 0x00000000000003e8, 1894 (Float[1,128,1,1]) -> 1897 (Int8[1,128,1,1])
Layer(CaskPooling): MaxPool_1811, Tactic: 0x1f6c40e3e09ec730, 1922 (Int8[1,128:4,3,3]) -> 1925 (Int8[1,128:4,1,1])
Layer(Reformat): QuantizeLinear_1936, Tactic: 0x00000000000003e8, 2044 (Float[1,128,1,1]) -> 2047 (Int8[1,128,1,1])
Layer(CaskPooling): MaxPool_1961, Tactic: 0x94215b398b8eb3ba, 2072 (Int8[1,128:32,3,3]) -> 2075 (Int8[1,128:32,1,1])
Layer(Reformat): QuantizeLinear_2086, Tactic: 0x00000000000003e8, 2194 (Float[1,128,1,1]) -> 2197 (Int8[1,128,1,1])
Layer(CaskPooling): MaxPool_2111, Tactic: 0x1f6c40e3e09ec730, 2222 (Int8[1,128:4,3,3]) -> 2225 (Int8[1,128:4,1,1])
Layer(Reformat): QuantizeLinear_2236, Tactic: 0x00000000000003e8, 2344 (Float[1,128,1,1]) -> 2347 (Int8[1,128,1,1])
Layer(CaskPooling): MaxPool_2261, Tactic: 0x1f6c40e3e09ec730, 2372 (Int8[1,128:4,3,3]) -> 2375 (Int8[1,128:4,1,1])
Layer(Reformat): QuantizeLinear_2386, Tactic: 0x00000000000003e8, 2494 (Float[1,128,1,1]) -> 2497 (Int8[1,128,1,1])
Layer(CaskPooling): MaxPool_2411, Tactic: 0x1f6c40e3e09ec730, 2522 (Int8[1,128:4,3,3]) -> 2525 (Int8[1,128:4,1,1])
Layer(Reformat): QuantizeLinear_2536, Tactic: 0x00000000000003e8, 2644 (Float[1,128,1,1]) -> 2647 (Int8[1,128,1,1])
Layer(CaskPooling): MaxPool_2561, Tactic: 0x1f6c40e3e09ec730, 2672 (Int8[1,128:4,3,3]) -> 2675 (Int8[1,128:4,1,1])
Layer(CaskPooling): MaxPool_99, Tactic: 0xdd39499496c9cc38, 209 (Float[1,256,8,8]) -> 211 (Float[1,256,4,4])
Layer(Reformat): 179 copy, Tactic: 0x00000000000003e8, 179 (Float[1,128,4,4]) -> 211 (Float[1,128,4,4])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_0 + QuantizeLinear_293 + Conv_297 + Relu_298, Tactic: 0x0000000000000000, 398 (Int8[1,128,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_0 + QuantizeLinear_293 + Conv_297 + Relu_298 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_0 + QuantizeLinear_293 + Conv_297 + Relu_298, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_0 + QuantizeLinear_293 + Conv_297 + Relu_298 (Int8[1,128:32,1,1]) -> 412 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_1 + QuantizeLinear_321 + Conv_325 + Relu_326, Tactic: 0x0000000000000000, 426 (Int8[1,128:4,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_1 + QuantizeLinear_321 + Conv_325 + Relu_326 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_1 + QuantizeLinear_321 + Conv_325 + Relu_326, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_1 + QuantizeLinear_321 + Conv_325 + Relu_326 (Int8[1,128:32,1,1]) -> 440 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_2 + QuantizeLinear_442 + Conv_446 + Relu_447, Tactic: 0x0000000000000000, 547 (Int8[1,128,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_2 + QuantizeLinear_442 + Conv_446 + Relu_447 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_2 + QuantizeLinear_442 + Conv_446 + Relu_447, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_2 + QuantizeLinear_442 + Conv_446 + Relu_447 (Int8[1,128:32,1,1]) -> 561 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_3 + QuantizeLinear_470 + Conv_474 + Relu_475, Tactic: 0x0000000000000000, 575 (Int8[1,128:4,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_3 + QuantizeLinear_470 + Conv_474 + Relu_475 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_3 + QuantizeLinear_470 + Conv_474 + Relu_475, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_3 + QuantizeLinear_470 + Conv_474 + Relu_475 (Int8[1,128:32,1,1]) -> 589 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_4 + QuantizeLinear_592 + Conv_596 + Relu_597, Tactic: 0x0000000000000000, 697 (Int8[1,128,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_4 + QuantizeLinear_592 + Conv_596 + Relu_597 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_4 + QuantizeLinear_592 + Conv_596 + Relu_597, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_4 + QuantizeLinear_592 + Conv_596 + Relu_597 (Int8[1,128:32,1,1]) -> 711 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_5 + QuantizeLinear_620 + Conv_624 + Relu_625, Tactic: 0x0000000000000000, 725 (Int8[1,128:4,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_5 + QuantizeLinear_620 + Conv_624 + Relu_625 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_5 + QuantizeLinear_620 + Conv_624 + Relu_625, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_5 + QuantizeLinear_620 + Conv_624 + Relu_625 (Int8[1,128:32,1,1]) -> 739 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_6 + QuantizeLinear_742 + Conv_746 + Relu_747, Tactic: 0x0000000000000000, 847 (Int8[1,128,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_6 + QuantizeLinear_742 + Conv_746 + Relu_747 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_6 + QuantizeLinear_742 + Conv_746 + Relu_747, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_6 + QuantizeLinear_742 + Conv_746 + Relu_747 (Int8[1,128:32,1,1]) -> 861 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_7 + QuantizeLinear_770 + Conv_774 + Relu_775, Tactic: 0x0000000000000000, 875 (Int8[1,128:4,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_7 + QuantizeLinear_770 + Conv_774 + Relu_775 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_7 + QuantizeLinear_770 + Conv_774 + Relu_775, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_7 + QuantizeLinear_770 + Conv_774 + Relu_775 (Int8[1,128:32,1,1]) -> 889 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_8 + QuantizeLinear_892 + Conv_896 + Relu_897, Tactic: 0x0000000000000000, 997 (Int8[1,128,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_8 + QuantizeLinear_892 + Conv_896 + Relu_897 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_8 + QuantizeLinear_892 + Conv_896 + Relu_897, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_8 + QuantizeLinear_892 + Conv_896 + Relu_897 (Int8[1,128:32,1,1]) -> 1011 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_9 + QuantizeLinear_920 + Conv_924 + Relu_925, Tactic: 0x0000000000000000, 1025 (Int8[1,128:4,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_9 + QuantizeLinear_920 + Conv_924 + Relu_925 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_9 + QuantizeLinear_920 + Conv_924 + Relu_925, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_9 + QuantizeLinear_920 + Conv_924 + Relu_925 (Int8[1,128:32,1,1]) -> 1039 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_10 + QuantizeLinear_1042 + Conv_1046 + Relu_1047, Tactic: 0x0000000000000000, 1147 (Int8[1,128,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_10 + QuantizeLinear_1042 + Conv_1046 + Relu_1047 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_10 + QuantizeLinear_1042 + Conv_1046 + Relu_1047, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_10 + QuantizeLinear_1042 + Conv_1046 + Relu_1047 (Int8[1,128:32,1,1]) -> 1161 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_11 + QuantizeLinear_1070 + Conv_1074 + Relu_1075, Tactic: 0x0000000000000000, 1175 (Int8[1,128:4,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_11 + QuantizeLinear_1070 + Conv_1074 + Relu_1075 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_11 + QuantizeLinear_1070 + Conv_1074 + Relu_1075, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_11 + QuantizeLinear_1070 + Conv_1074 + Relu_1075 (Int8[1,128:32,1,1]) -> 1189 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_12 + QuantizeLinear_1192 + Conv_1196 + Relu_1197, Tactic: 0x0000000000000000, 1297 (Int8[1,128,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_12 + QuantizeLinear_1192 + Conv_1196 + Relu_1197 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_12 + QuantizeLinear_1192 + Conv_1196 + Relu_1197, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_12 + QuantizeLinear_1192 + Conv_1196 + Relu_1197 (Int8[1,128:32,1,1]) -> 1311 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_13 + QuantizeLinear_1220 + Conv_1224 + Relu_1225, Tactic: 0x0000000000000000, 1325 (Int8[1,128:4,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_13 + QuantizeLinear_1220 + Conv_1224 + Relu_1225 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_13 + QuantizeLinear_1220 + Conv_1224 + Relu_1225, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_13 + QuantizeLinear_1220 + Conv_1224 + Relu_1225 (Int8[1,128:32,1,1]) -> 1339 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_14 + QuantizeLinear_1342 + Conv_1346 + Relu_1347, Tactic: 0x0000000000000000, 1447 (Int8[1,128,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_14 + QuantizeLinear_1342 + Conv_1346 + Relu_1347 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_14 + QuantizeLinear_1342 + Conv_1346 + Relu_1347, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_14 + QuantizeLinear_1342 + Conv_1346 + Relu_1347 (Int8[1,128:32,1,1]) -> 1461 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_15 + QuantizeLinear_1370 + Conv_1374 + Relu_1375, Tactic: 0x0000000000000000, 1475 (Int8[1,128:4,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_15 + QuantizeLinear_1370 + Conv_1374 + Relu_1375 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_15 + QuantizeLinear_1370 + Conv_1374 + Relu_1375, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_15 + QuantizeLinear_1370 + Conv_1374 + Relu_1375 (Int8[1,128:32,1,1]) -> 1489 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_16 + QuantizeLinear_1492 + Conv_1496 + Relu_1497, Tactic: 0x0000000000000000, 1597 (Int8[1,128,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_16 + QuantizeLinear_1492 + Conv_1496 + Relu_1497 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_16 + QuantizeLinear_1492 + Conv_1496 + Relu_1497, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_16 + QuantizeLinear_1492 + Conv_1496 + Relu_1497 (Int8[1,128:32,1,1]) -> 1611 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_17 + QuantizeLinear_1520 + Conv_1524 + Relu_1525, Tactic: 0x0000000000000000, 1625 (Int8[1,128:4,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_17 + QuantizeLinear_1520 + Conv_1524 + Relu_1525 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_17 + QuantizeLinear_1520 + Conv_1524 + Relu_1525, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_17 + QuantizeLinear_1520 + Conv_1524 + Relu_1525 (Int8[1,128:32,1,1]) -> 1639 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_18 + QuantizeLinear_1642 + Conv_1646 + Relu_1647, Tactic: 0x0000000000000000, 1747 (Int8[1,128,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_18 + QuantizeLinear_1642 + Conv_1646 + Relu_1647 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_18 + QuantizeLinear_1642 + Conv_1646 + Relu_1647, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_18 + QuantizeLinear_1642 + Conv_1646 + Relu_1647 (Int8[1,128:32,1,1]) -> 1761 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_19 + QuantizeLinear_1670 + Conv_1674 + Relu_1675, Tactic: 0x0000000000000000, 1775 (Int8[1,128:4,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_19 + QuantizeLinear_1670 + Conv_1674 + Relu_1675 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_19 + QuantizeLinear_1670 + Conv_1674 + Relu_1675, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_19 + QuantizeLinear_1670 + Conv_1674 + Relu_1675 (Int8[1,128:32,1,1]) -> 1789 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_20 + QuantizeLinear_1792 + Conv_1796 + Relu_1797, Tactic: 0x0000000000000000, 1897 (Int8[1,128,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_20 + QuantizeLinear_1792 + Conv_1796 + Relu_1797 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_20 + QuantizeLinear_1792 + Conv_1796 + Relu_1797, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_20 + QuantizeLinear_1792 + Conv_1796 + Relu_1797 (Int8[1,128:32,1,1]) -> 1911 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_21 + QuantizeLinear_1820 + Conv_1824 + Relu_1825, Tactic: 0x0000000000000000, 1925 (Int8[1,128:4,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_21 + QuantizeLinear_1820 + Conv_1824 + Relu_1825 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_21 + QuantizeLinear_1820 + Conv_1824 + Relu_1825, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_21 + QuantizeLinear_1820 + Conv_1824 + Relu_1825 (Int8[1,128:32,1,1]) -> 1939 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_22 + QuantizeLinear_1942 + Conv_1946 + Relu_1947, Tactic: 0x0000000000000000, 2047 (Int8[1,128,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_22 + QuantizeLinear_1942 + Conv_1946 + Relu_1947 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_22 + QuantizeLinear_1942 + Conv_1946 + Relu_1947, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_22 + QuantizeLinear_1942 + Conv_1946 + Relu_1947 (Int8[1,128:32,1,1]) -> 2061 (Int8[1,8:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_23 + QuantizeLinear_1970 + Conv_1974 + Relu_1975, Tactic: 0x3912ca79eb9a8be1, 2075 (Int8[1,128:32,1,1]) -> 2089 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_24 + QuantizeLinear_2092 + Conv_2096 + Relu_2097, Tactic: 0x0000000000000000, 2197 (Int8[1,128,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_24 + QuantizeLinear_2092 + Conv_2096 + Relu_2097 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_24 + QuantizeLinear_2092 + Conv_2096 + Relu_2097, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_24 + QuantizeLinear_2092 + Conv_2096 + Relu_2097 (Int8[1,128:32,1,1]) -> 2211 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_25 + QuantizeLinear_2120 + Conv_2124 + Relu_2125, Tactic: 0x0000000000000000, 2225 (Int8[1,128:4,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_25 + QuantizeLinear_2120 + Conv_2124 + Relu_2125 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_25 + QuantizeLinear_2120 + Conv_2124 + Relu_2125, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_25 + QuantizeLinear_2120 + Conv_2124 + Relu_2125 (Int8[1,128:32,1,1]) -> 2239 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_26 + QuantizeLinear_2242 + Conv_2246 + Relu_2247, Tactic: 0x0000000000000000, 2347 (Int8[1,128,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_26 + QuantizeLinear_2242 + Conv_2246 + Relu_2247 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_26 + QuantizeLinear_2242 + Conv_2246 + Relu_2247, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_26 + QuantizeLinear_2242 + Conv_2246 + Relu_2247 (Int8[1,128:32,1,1]) -> 2361 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_27 + QuantizeLinear_2270 + Conv_2274 + Relu_2275, Tactic: 0x0000000000000000, 2375 (Int8[1,128:4,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_27 + QuantizeLinear_2270 + Conv_2274 + Relu_2275 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_27 + QuantizeLinear_2270 + Conv_2274 + Relu_2275, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_27 + QuantizeLinear_2270 + Conv_2274 + Relu_2275 (Int8[1,128:32,1,1]) -> 2389 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_28 + QuantizeLinear_2392 + Conv_2396 + Relu_2397, Tactic: 0x0000000000000000, 2497 (Int8[1,128,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_28 + QuantizeLinear_2392 + Conv_2396 + Relu_2397 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_28 + QuantizeLinear_2392 + Conv_2396 + Relu_2397, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_28 + QuantizeLinear_2392 + Conv_2396 + Relu_2397 (Int8[1,128:32,1,1]) -> 2511 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_29 + QuantizeLinear_2420 + Conv_2424 + Relu_2425, Tactic: 0x0000000000000000, 2525 (Int8[1,128:4,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_29 + QuantizeLinear_2420 + Conv_2424 + Relu_2425 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_29 + QuantizeLinear_2420 + Conv_2424 + Relu_2425, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_29 + QuantizeLinear_2420 + Conv_2424 + Relu_2425 (Int8[1,128:32,1,1]) -> 2539 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_30 + QuantizeLinear_2542 + Conv_2546 + Relu_2547, Tactic: 0x0000000000000000, 2647 (Int8[1,128,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_30 + QuantizeLinear_2542 + Conv_2546 + Relu_2547 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_30 + QuantizeLinear_2542 + Conv_2546 + Relu_2547, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_30 + QuantizeLinear_2542 + Conv_2546 + Relu_2547 (Int8[1,128:32,1,1]) -> 2661 (Int8[1,8:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to patchattention_channel.fc.0.weight_clone_31 + QuantizeLinear_2570 + Conv_2574 + Relu_2575, Tactic: 0x0000000000000000, 2675 (Int8[1,128:4,1,1]) -> Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_31 + QuantizeLinear_2570 + Conv_2574 + Relu_2575 (Int8[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.0.weight_clone_31 + QuantizeLinear_2570 + Conv_2574 + Relu_2575, Tactic: 0x3912ca79eb9a8be1, Reformatted Input Tensor 0 to patchattention_channel.fc.0.weight_clone_31 + QuantizeLinear_2570 + Conv_2574 + Relu_2575 (Int8[1,128:32,1,1]) -> 2689 (Int8[1,8:32,1,1])
Layer(Reduce): ReduceMean_101, Tactic: 0x0000000000000005, 211 (Float[1,384,4,4]) -> 212 (Float[1,1,4,4])
Layer(Reduce): ReduceMax_102, Tactic: 0x0000000000000005, 211 (Float[1,384,4,4]) -> 213 (Float[1,1,4,4])
Layer(Reformat): QuantizeLinear_106_clone_1, Tactic: 0x00000000000003e8, 213 (Float[1,1,4,4]) -> 217 (Int8[1,1:4,4,4])
Layer(Reformat): QuantizeLinear_106_clone_0, Tactic: 0x00000000000003e8, 212 (Float[1,1,4,4]) -> Concat_103_212_clone_0 (Int8[1,1,4,4])
Layer(Reformat): Concat_103_212_clone_0 copy, Tactic: 0x00000000000003e8, Concat_103_212_clone_0 (Int8[1,1,4,4]) -> 217 (Int8[1,1:4,4,4])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_0 + QuantizeLinear_307 + Conv_311, Tactic: 0x445983715412fbda, 412 (Int8[1,8:32,1,1]) -> 422 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_1 + QuantizeLinear_335 + Conv_339, Tactic: 0x445983715412fbda, 440 (Int8[1,8:32,1,1]) -> 450 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_2 + QuantizeLinear_456 + Conv_460, Tactic: 0x445983715412fbda, 561 (Int8[1,8:32,1,1]) -> 571 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_3 + QuantizeLinear_484 + Conv_488, Tactic: 0x445983715412fbda, 589 (Int8[1,8:32,1,1]) -> 599 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_4 + QuantizeLinear_606 + Conv_610, Tactic: 0x445983715412fbda, 711 (Int8[1,8:32,1,1]) -> 721 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_5 + QuantizeLinear_634 + Conv_638, Tactic: 0x445983715412fbda, 739 (Int8[1,8:32,1,1]) -> 749 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_6 + QuantizeLinear_756 + Conv_760, Tactic: 0x445983715412fbda, 861 (Int8[1,8:32,1,1]) -> 871 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_7 + QuantizeLinear_784 + Conv_788, Tactic: 0x445983715412fbda, 889 (Int8[1,8:32,1,1]) -> 899 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_8 + QuantizeLinear_906 + Conv_910, Tactic: 0x445983715412fbda, 1011 (Int8[1,8:32,1,1]) -> 1021 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_9 + QuantizeLinear_934 + Conv_938, Tactic: 0x445983715412fbda, 1039 (Int8[1,8:32,1,1]) -> 1049 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_10 + QuantizeLinear_1056 + Conv_1060, Tactic: 0x445983715412fbda, 1161 (Int8[1,8:32,1,1]) -> 1171 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_11 + QuantizeLinear_1084 + Conv_1088, Tactic: 0x445983715412fbda, 1189 (Int8[1,8:32,1,1]) -> 1199 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_12 + QuantizeLinear_1206 + Conv_1210, Tactic: 0x445983715412fbda, 1311 (Int8[1,8:32,1,1]) -> 1321 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_13 + QuantizeLinear_1234 + Conv_1238, Tactic: 0x445983715412fbda, 1339 (Int8[1,8:32,1,1]) -> 1349 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_14 + QuantizeLinear_1356 + Conv_1360, Tactic: 0x445983715412fbda, 1461 (Int8[1,8:32,1,1]) -> 1471 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_15 + QuantizeLinear_1384 + Conv_1388, Tactic: 0x445983715412fbda, 1489 (Int8[1,8:32,1,1]) -> 1499 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_16 + QuantizeLinear_1506 + Conv_1510, Tactic: 0x445983715412fbda, 1611 (Int8[1,8:32,1,1]) -> 1621 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_17 + QuantizeLinear_1534 + Conv_1538, Tactic: 0x445983715412fbda, 1639 (Int8[1,8:32,1,1]) -> 1649 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_18 + QuantizeLinear_1656 + Conv_1660, Tactic: 0x445983715412fbda, 1761 (Int8[1,8:32,1,1]) -> 1771 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_19 + QuantizeLinear_1684 + Conv_1688, Tactic: 0x445983715412fbda, 1789 (Int8[1,8:32,1,1]) -> 1799 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_20 + QuantizeLinear_1806 + Conv_1810, Tactic: 0x445983715412fbda, 1911 (Int8[1,8:32,1,1]) -> 1921 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_21 + QuantizeLinear_1834 + Conv_1838, Tactic: 0x445983715412fbda, 1939 (Int8[1,8:32,1,1]) -> 1949 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_22 + QuantizeLinear_1956 + Conv_1960, Tactic: 0x445983715412fbda, 2061 (Int8[1,8:32,1,1]) -> 2071 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_23 + QuantizeLinear_1984 + Conv_1988, Tactic: 0x445983715412fbda, 2089 (Int8[1,8:32,1,1]) -> 2099 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_24 + QuantizeLinear_2106 + Conv_2110, Tactic: 0x445983715412fbda, 2211 (Int8[1,8:32,1,1]) -> 2221 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_25 + QuantizeLinear_2134 + Conv_2138, Tactic: 0x445983715412fbda, 2239 (Int8[1,8:32,1,1]) -> 2249 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_26 + QuantizeLinear_2256 + Conv_2260, Tactic: 0x445983715412fbda, 2361 (Int8[1,8:32,1,1]) -> 2371 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_27 + QuantizeLinear_2284 + Conv_2288, Tactic: 0x445983715412fbda, 2389 (Int8[1,8:32,1,1]) -> 2399 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_28 + QuantizeLinear_2406 + Conv_2410, Tactic: 0x445983715412fbda, 2511 (Int8[1,8:32,1,1]) -> 2521 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_29 + QuantizeLinear_2434 + Conv_2438, Tactic: 0x445983715412fbda, 2539 (Int8[1,8:32,1,1]) -> 2549 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_30 + QuantizeLinear_2556 + Conv_2560, Tactic: 0x445983715412fbda, 2661 (Int8[1,8:32,1,1]) -> 2671 (Float[1,128:32,1,1])
Layer(CaskConvolution): patchattention_channel.fc.2.weight_clone_31 + QuantizeLinear_2584 + Conv_2588, Tactic: 0x445983715412fbda, 2689 (Int8[1,8:32,1,1]) -> 2699 (Float[1,128:32,1,1])
Layer(CaskConvolution): attention_spatial.conv1.weight + QuantizeLinear_112 + Conv_116, Tactic: 0x1b0534177b414e71, 217 (Int8[1,2:4,4,4]) -> 227 (Float[1,1,4,4])
Layer(PointWiseV2): PWN(Sigmoid_117, Mul_118), Tactic: 0x0000000000000009, 227 (Float[1,1,4,4]), 211 (Float[1,384,4,4]) -> 229 (Float[1,384,4,4])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to PWN(PWN(Add_340, Sigmoid_341), Mul_342), Tactic: 0x0000000000000000, 422 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 0 to PWN(PWN(Add_340, Sigmoid_341), Mul_342) (Float[1,128,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 1 to PWN(PWN(Add_340, Sigmoid_341), Mul_342), Tactic: 0x0000000000000000, 450 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 1 to PWN(PWN(Add_340, Sigmoid_341), Mul_342) (Float[1,128,1,1])
Layer(PointWiseV2): PWN(PWN(Add_340, Sigmoid_341), Mul_342), Tactic: 0x0000000000000001, Reformatted Input Tensor 0 to PWN(PWN(Add_340, Sigmoid_341), Mul_342) (Float[1,128,1,1]), Reformatted Input Tensor 1 to PWN(PWN(Add_340, Sigmoid_341), Mul_342) (Float[1,128,1,1]), 394 (Float[1,128,3,3]) -> 2703 (Float[1,128,3,3])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to PWN(PWN(Add_489, Sigmoid_490), Mul_491), Tactic: 0x0000000000000000, 571 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 0 to PWN(PWN(Add_489, Sigmoid_490), Mul_491) (Float[1,128,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 1 to PWN(PWN(Add_489, Sigmoid_490), Mul_491), Tactic: 0x0000000000000000, 599 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 1 to PWN(PWN(Add_489, Sigmoid_490), Mul_491) (Float[1,128,1,1])
Layer(PointWiseV2): PWN(PWN(Add_489, Sigmoid_490), Mul_491), Tactic: 0x0000000000000001, Reformatted Input Tensor 0 to PWN(PWN(Add_489, Sigmoid_490), Mul_491) (Float[1,128,1,1]), Reformatted Input Tensor 1 to PWN(PWN(Add_489, Sigmoid_490), Mul_491) (Float[1,128,1,1]), 543 (Float[1,128,3,3]) -> 2703 (Float[1,128,3,3])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to PWN(PWN(Add_639, Sigmoid_640), Mul_641), Tactic: 0x0000000000000000, 721 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 0 to PWN(PWN(Add_639, Sigmoid_640), Mul_641) (Float[1,128,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 1 to PWN(PWN(Add_639, Sigmoid_640), Mul_641), Tactic: 0x0000000000000000, 749 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 1 to PWN(PWN(Add_639, Sigmoid_640), Mul_641) (Float[1,128,1,1])
Layer(PointWiseV2): PWN(PWN(Add_639, Sigmoid_640), Mul_641), Tactic: 0x0000000000000001, Reformatted Input Tensor 0 to PWN(PWN(Add_639, Sigmoid_640), Mul_641) (Float[1,128,1,1]), Reformatted Input Tensor 1 to PWN(PWN(Add_639, Sigmoid_640), Mul_641) (Float[1,128,1,1]), 693 (Float[1,128,3,3]) -> 2703 (Float[1,128,3,3])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to PWN(PWN(Add_789, Sigmoid_790), Mul_791), Tactic: 0x0000000000000000, 871 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 0 to PWN(PWN(Add_789, Sigmoid_790), Mul_791) (Float[1,128,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 1 to PWN(PWN(Add_789, Sigmoid_790), Mul_791), Tactic: 0x0000000000000000, 899 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 1 to PWN(PWN(Add_789, Sigmoid_790), Mul_791) (Float[1,128,1,1])
Layer(PointWiseV2): PWN(PWN(Add_789, Sigmoid_790), Mul_791), Tactic: 0x0000000000000001, Reformatted Input Tensor 0 to PWN(PWN(Add_789, Sigmoid_790), Mul_791) (Float[1,128,1,1]), Reformatted Input Tensor 1 to PWN(PWN(Add_789, Sigmoid_790), Mul_791) (Float[1,128,1,1]), 843 (Float[1,128,3,3]) -> 2703 (Float[1,128,3,3])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to PWN(PWN(Add_939, Sigmoid_940), Mul_941), Tactic: 0x0000000000000000, 1021 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 0 to PWN(PWN(Add_939, Sigmoid_940), Mul_941) (Float[1,128,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 1 to PWN(PWN(Add_939, Sigmoid_940), Mul_941), Tactic: 0x0000000000000000, 1049 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 1 to PWN(PWN(Add_939, Sigmoid_940), Mul_941) (Float[1,128,1,1])
Layer(PointWiseV2): PWN(PWN(Add_939, Sigmoid_940), Mul_941), Tactic: 0x0000000000000001, Reformatted Input Tensor 0 to PWN(PWN(Add_939, Sigmoid_940), Mul_941) (Float[1,128,1,1]), Reformatted Input Tensor 1 to PWN(PWN(Add_939, Sigmoid_940), Mul_941) (Float[1,128,1,1]), 993 (Float[1,128,3,3]) -> 2703 (Float[1,128,3,3])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to PWN(PWN(Add_1089, Sigmoid_1090), Mul_1091), Tactic: 0x0000000000000000, 1171 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 0 to PWN(PWN(Add_1089, Sigmoid_1090), Mul_1091) (Float[1,128,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 1 to PWN(PWN(Add_1089, Sigmoid_1090), Mul_1091), Tactic: 0x0000000000000000, 1199 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 1 to PWN(PWN(Add_1089, Sigmoid_1090), Mul_1091) (Float[1,128,1,1])
Layer(PointWiseV2): PWN(PWN(Add_1089, Sigmoid_1090), Mul_1091), Tactic: 0x0000000000000001, Reformatted Input Tensor 0 to PWN(PWN(Add_1089, Sigmoid_1090), Mul_1091) (Float[1,128,1,1]), Reformatted Input Tensor 1 to PWN(PWN(Add_1089, Sigmoid_1090), Mul_1091) (Float[1,128,1,1]), 1143 (Float[1,128,3,3]) -> 2703 (Float[1,128,3,3])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to PWN(PWN(Add_1239, Sigmoid_1240), Mul_1241), Tactic: 0x0000000000000000, 1321 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 0 to PWN(PWN(Add_1239, Sigmoid_1240), Mul_1241) (Float[1,128,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 1 to PWN(PWN(Add_1239, Sigmoid_1240), Mul_1241), Tactic: 0x0000000000000000, 1349 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 1 to PWN(PWN(Add_1239, Sigmoid_1240), Mul_1241) (Float[1,128,1,1])
Layer(PointWiseV2): PWN(PWN(Add_1239, Sigmoid_1240), Mul_1241), Tactic: 0x0000000000000001, Reformatted Input Tensor 0 to PWN(PWN(Add_1239, Sigmoid_1240), Mul_1241) (Float[1,128,1,1]), Reformatted Input Tensor 1 to PWN(PWN(Add_1239, Sigmoid_1240), Mul_1241) (Float[1,128,1,1]), 1293 (Float[1,128,3,3]) -> 2703 (Float[1,128,3,3])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to PWN(PWN(Add_1389, Sigmoid_1390), Mul_1391), Tactic: 0x0000000000000000, 1471 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 0 to PWN(PWN(Add_1389, Sigmoid_1390), Mul_1391) (Float[1,128,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 1 to PWN(PWN(Add_1389, Sigmoid_1390), Mul_1391), Tactic: 0x0000000000000000, 1499 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 1 to PWN(PWN(Add_1389, Sigmoid_1390), Mul_1391) (Float[1,128,1,1])
Layer(PointWiseV2): PWN(PWN(Add_1389, Sigmoid_1390), Mul_1391), Tactic: 0x0000000000000001, Reformatted Input Tensor 0 to PWN(PWN(Add_1389, Sigmoid_1390), Mul_1391) (Float[1,128,1,1]), Reformatted Input Tensor 1 to PWN(PWN(Add_1389, Sigmoid_1390), Mul_1391) (Float[1,128,1,1]), 1443 (Float[1,128,3,3]) -> 2703 (Float[1,128,3,3])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to PWN(PWN(Add_1539, Sigmoid_1540), Mul_1541), Tactic: 0x0000000000000000, 1621 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 0 to PWN(PWN(Add_1539, Sigmoid_1540), Mul_1541) (Float[1,128,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 1 to PWN(PWN(Add_1539, Sigmoid_1540), Mul_1541), Tactic: 0x0000000000000000, 1649 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 1 to PWN(PWN(Add_1539, Sigmoid_1540), Mul_1541) (Float[1,128,1,1])
Layer(PointWiseV2): PWN(PWN(Add_1539, Sigmoid_1540), Mul_1541), Tactic: 0x0000000000000001, Reformatted Input Tensor 0 to PWN(PWN(Add_1539, Sigmoid_1540), Mul_1541) (Float[1,128,1,1]), Reformatted Input Tensor 1 to PWN(PWN(Add_1539, Sigmoid_1540), Mul_1541) (Float[1,128,1,1]), 1593 (Float[1,128,3,3]) -> 2703 (Float[1,128,3,3])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to PWN(PWN(Add_1689, Sigmoid_1690), Mul_1691), Tactic: 0x0000000000000000, 1771 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 0 to PWN(PWN(Add_1689, Sigmoid_1690), Mul_1691) (Float[1,128,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 1 to PWN(PWN(Add_1689, Sigmoid_1690), Mul_1691), Tactic: 0x0000000000000000, 1799 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 1 to PWN(PWN(Add_1689, Sigmoid_1690), Mul_1691) (Float[1,128,1,1])
Layer(PointWiseV2): PWN(PWN(Add_1689, Sigmoid_1690), Mul_1691), Tactic: 0x0000000000000001, Reformatted Input Tensor 0 to PWN(PWN(Add_1689, Sigmoid_1690), Mul_1691) (Float[1,128,1,1]), Reformatted Input Tensor 1 to PWN(PWN(Add_1689, Sigmoid_1690), Mul_1691) (Float[1,128,1,1]), 1743 (Float[1,128,3,3]) -> 2703 (Float[1,128,3,3])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to PWN(PWN(Add_1839, Sigmoid_1840), Mul_1841), Tactic: 0x0000000000000000, 1921 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 0 to PWN(PWN(Add_1839, Sigmoid_1840), Mul_1841) (Float[1,128,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 1 to PWN(PWN(Add_1839, Sigmoid_1840), Mul_1841), Tactic: 0x0000000000000000, 1949 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 1 to PWN(PWN(Add_1839, Sigmoid_1840), Mul_1841) (Float[1,128,1,1])
Layer(PointWiseV2): PWN(PWN(Add_1839, Sigmoid_1840), Mul_1841), Tactic: 0x0000000000000001, Reformatted Input Tensor 0 to PWN(PWN(Add_1839, Sigmoid_1840), Mul_1841) (Float[1,128,1,1]), Reformatted Input Tensor 1 to PWN(PWN(Add_1839, Sigmoid_1840), Mul_1841) (Float[1,128,1,1]), 1893 (Float[1,128,3,3]) -> 2703 (Float[1,128,3,3])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to PWN(PWN(Add_1989, Sigmoid_1990), Mul_1991), Tactic: 0x0000000000000000, 2071 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 0 to PWN(PWN(Add_1989, Sigmoid_1990), Mul_1991) (Float[1,128,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 1 to PWN(PWN(Add_1989, Sigmoid_1990), Mul_1991), Tactic: 0x0000000000000000, 2099 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 1 to PWN(PWN(Add_1989, Sigmoid_1990), Mul_1991) (Float[1,128,1,1])
Layer(PointWiseV2): PWN(PWN(Add_1989, Sigmoid_1990), Mul_1991), Tactic: 0x0000000000000001, Reformatted Input Tensor 0 to PWN(PWN(Add_1989, Sigmoid_1990), Mul_1991) (Float[1,128,1,1]), Reformatted Input Tensor 1 to PWN(PWN(Add_1989, Sigmoid_1990), Mul_1991) (Float[1,128,1,1]), 2043 (Float[1,128,3,3]) -> 2703 (Float[1,128,3,3])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to PWN(PWN(Add_2139, Sigmoid_2140), Mul_2141), Tactic: 0x0000000000000000, 2221 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 0 to PWN(PWN(Add_2139, Sigmoid_2140), Mul_2141) (Float[1,128,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 1 to PWN(PWN(Add_2139, Sigmoid_2140), Mul_2141), Tactic: 0x0000000000000000, 2249 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 1 to PWN(PWN(Add_2139, Sigmoid_2140), Mul_2141) (Float[1,128,1,1])
Layer(PointWiseV2): PWN(PWN(Add_2139, Sigmoid_2140), Mul_2141), Tactic: 0x0000000000000001, Reformatted Input Tensor 0 to PWN(PWN(Add_2139, Sigmoid_2140), Mul_2141) (Float[1,128,1,1]), Reformatted Input Tensor 1 to PWN(PWN(Add_2139, Sigmoid_2140), Mul_2141) (Float[1,128,1,1]), 2193 (Float[1,128,3,3]) -> 2703 (Float[1,128,3,3])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to PWN(PWN(Add_2289, Sigmoid_2290), Mul_2291), Tactic: 0x0000000000000000, 2371 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 0 to PWN(PWN(Add_2289, Sigmoid_2290), Mul_2291) (Float[1,128,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 1 to PWN(PWN(Add_2289, Sigmoid_2290), Mul_2291), Tactic: 0x0000000000000000, 2399 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 1 to PWN(PWN(Add_2289, Sigmoid_2290), Mul_2291) (Float[1,128,1,1])
Layer(PointWiseV2): PWN(PWN(Add_2289, Sigmoid_2290), Mul_2291), Tactic: 0x0000000000000001, Reformatted Input Tensor 0 to PWN(PWN(Add_2289, Sigmoid_2290), Mul_2291) (Float[1,128,1,1]), Reformatted Input Tensor 1 to PWN(PWN(Add_2289, Sigmoid_2290), Mul_2291) (Float[1,128,1,1]), 2343 (Float[1,128,3,3]) -> 2703 (Float[1,128,3,3])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to PWN(PWN(Add_2439, Sigmoid_2440), Mul_2441), Tactic: 0x0000000000000000, 2521 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 0 to PWN(PWN(Add_2439, Sigmoid_2440), Mul_2441) (Float[1,128,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 1 to PWN(PWN(Add_2439, Sigmoid_2440), Mul_2441), Tactic: 0x0000000000000000, 2549 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 1 to PWN(PWN(Add_2439, Sigmoid_2440), Mul_2441) (Float[1,128,1,1])
Layer(PointWiseV2): PWN(PWN(Add_2439, Sigmoid_2440), Mul_2441), Tactic: 0x0000000000000001, Reformatted Input Tensor 0 to PWN(PWN(Add_2439, Sigmoid_2440), Mul_2441) (Float[1,128,1,1]), Reformatted Input Tensor 1 to PWN(PWN(Add_2439, Sigmoid_2440), Mul_2441) (Float[1,128,1,1]), 2493 (Float[1,128,3,3]) -> 2703 (Float[1,128,3,3])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to PWN(PWN(Add_2589, Sigmoid_2590), Mul_2591), Tactic: 0x0000000000000000, 2671 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 0 to PWN(PWN(Add_2589, Sigmoid_2590), Mul_2591) (Float[1,128,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 1 to PWN(PWN(Add_2589, Sigmoid_2590), Mul_2591), Tactic: 0x0000000000000000, 2699 (Float[1,128:32,1,1]) -> Reformatted Input Tensor 1 to PWN(PWN(Add_2589, Sigmoid_2590), Mul_2591) (Float[1,128,1,1])
Layer(PointWiseV2): PWN(PWN(Add_2589, Sigmoid_2590), Mul_2591), Tactic: 0x0000000000000001, Reformatted Input Tensor 0 to PWN(PWN(Add_2589, Sigmoid_2590), Mul_2591) (Float[1,128,1,1]), Reformatted Input Tensor 1 to PWN(PWN(Add_2589, Sigmoid_2590), Mul_2591) (Float[1,128,1,1]), 2643 (Float[1,128,3,3]) -> 2703 (Float[1,128,3,3])
Layer(CaskPooling): GlobalAveragePool_119, Tactic: 0x933eceba7b866d59, 229 (Float[1,384,4,4]) -> 230 (Float[1,384,1,1])
Layer(Reformat): QuantizeLinear_150, Tactic: 0x00000000000003e8, 229 (Float[1,384,4,4]) -> 258 (Int8[1,384:4,4,4])
Layer(Reformat): QuantizeLinear_122, Tactic: 0x00000000000003e8, 230 (Float[1,384,1,1]) -> 233 (Int8[1,384,1,1])
Layer(CaskPooling): MaxPool_147, Tactic: 0x1f6c40e3e09ec730, 258 (Int8[1,384:4,4,4]) -> 261 (Int8[1,384:4,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to attention_channel.fc.0.weight_clone_0 + QuantizeLinear_128 + Conv_132 + Relu_133, Tactic: 0x0000000000000000, 233 (Int8[1,384,1,1]) -> Reformatted Input Tensor 0 to attention_channel.fc.0.weight_clone_0 + QuantizeLinear_128 + Conv_132 + Relu_133 (Int8[1,384:32,1,1])
Layer(CaskConvolution): attention_channel.fc.0.weight_clone_0 + QuantizeLinear_128 + Conv_132 + Relu_133, Tactic: 0xcae7b5888d47fe1f, Reformatted Input Tensor 0 to attention_channel.fc.0.weight_clone_0 + QuantizeLinear_128 + Conv_132 + Relu_133 (Int8[1,384:32,1,1]) -> 247 (Int8[1,24:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to attention_channel.fc.0.weight_clone_1 + QuantizeLinear_156 + Conv_160 + Relu_161, Tactic: 0x0000000000000000, 261 (Int8[1,384:4,1,1]) -> Reformatted Input Tensor 0 to attention_channel.fc.0.weight_clone_1 + QuantizeLinear_156 + Conv_160 + Relu_161 (Int8[1,384:32,1,1])
Layer(CaskConvolution): attention_channel.fc.0.weight_clone_1 + QuantizeLinear_156 + Conv_160 + Relu_161, Tactic: 0xcae7b5888d47fe1f, Reformatted Input Tensor 0 to attention_channel.fc.0.weight_clone_1 + QuantizeLinear_156 + Conv_160 + Relu_161 (Int8[1,384:32,1,1]) -> 275 (Int8[1,24:32,1,1])
Layer(CaskConvolution): attention_channel.fc.2.weight_clone_0 + QuantizeLinear_142 + Conv_146, Tactic: 0x65fbe45b4cb1d8a5, 247 (Int8[1,24:32,1,1]) -> 257 (Float[1,384:32,1,1])
Layer(CaskConvolution): attention_channel.fc.2.weight_clone_1 + QuantizeLinear_170 + Conv_174, Tactic: 0x65fbe45b4cb1d8a5, 275 (Int8[1,24:32,1,1]) -> 285 (Float[1,384:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to PWN(PWN(Add_175, Sigmoid_176), Mul_177), Tactic: 0x0000000000000000, 257 (Float[1,384:32,1,1]) -> Reformatted Input Tensor 0 to PWN(PWN(Add_175, Sigmoid_176), Mul_177) (Float[1,384,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 1 to PWN(PWN(Add_175, Sigmoid_176), Mul_177), Tactic: 0x0000000000000000, 285 (Float[1,384:32,1,1]) -> Reformatted Input Tensor 1 to PWN(PWN(Add_175, Sigmoid_176), Mul_177) (Float[1,384,1,1])
Layer(PointWiseV2): PWN(PWN(Add_175, Sigmoid_176), Mul_177), Tactic: 0x0000000000000000, Reformatted Input Tensor 0 to PWN(PWN(Add_175, Sigmoid_176), Mul_177) (Float[1,384,1,1]), Reformatted Input Tensor 1 to PWN(PWN(Add_175, Sigmoid_176), Mul_177) (Float[1,384,1,1]), 229 (Float[1,384,4,4]) -> 288 (Float[1,384,4,4])
Layer(CaskPooling): GlobalAveragePool_178, Tactic: 0x933eceba7b866d59, 288 (Float[1,384,4,4]) -> 289 (Float[1,384,1,1])
Layer(Reformat): QuantizeLinear_183, Tactic: 0x0000000000000000, 289 (Float[1,384,1,1]) -> 291 (Int8[1,384:32,1,1])
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to classfier1.weight + QuantizeLinear_189 + transpose_before_Gemm_193 + Gemm_193 + classfier1.bias + (Unnamed Layer* 202) [Shuffle] + unsqueeze_node_after_classfier1.bias + (Unnamed Layer* 202) [Shuffle]_(Unnamed Layer* 202) [Shuffle]_output + (Unnamed Layer* 203) [ElementWise], Tactic: 0x0000000000000000, 291 (Int8[1,384:32,1,1]) -> Reformatted Input Tensor 0 to classfier1.weight + QuantizeLinear_189 + transpose_before_Gemm_193 + Gemm_193 + classfier1.bias + (Unnamed Layer* 202) [Shuffle] + unsqueeze_node_after_classfier1.bias + (Unnamed Layer* 202) [Shuffle]_(Unnamed Layer* 202) [Shuffle]_output + (Unnamed Layer* 203) [ElementWise] (Int8[1,384:4,1,1])
Layer(CaskConvolution): classfier1.weight + QuantizeLinear_189 + transpose_before_Gemm_193 + Gemm_193 + classfier1.bias + (Unnamed Layer* 202) [Shuffle] + unsqueeze_node_after_classfier1.bias + (Unnamed Layer* 202) [Shuffle]_(Unnamed Layer* 202) [Shuffle]_output + (Unnamed Layer* 203) [ElementWise], Tactic: 0xc073b0053ce90eac, Reformatted Input Tensor 0 to classfier1.weight + QuantizeLinear_189 + transpose_before_Gemm_193 + Gemm_193 + classfier1.bias + (Unnamed Layer* 202) [Shuffle] + unsqueeze_node_after_classfier1.bias + (Unnamed Layer* 202) [Shuffle]_(Unnamed Layer* 202) [Shuffle]_output + (Unnamed Layer* 203) [ElementWise] (Int8[1,384:4,1,1]) -> (Unnamed Layer* 203) [ElementWise]_out_tensor (Float[1,6,1,1])
Layer(CaskPooling): GlobalAveragePool_2593, Tactic: 0xba33c80addb15739, 2703 (Float[1,2048,3,3]) -> 2704 (Float[1,2048,1,1])
Layer(NoOp): copied_squeeze_after_(Unnamed Layer* 203) [ElementWise], Tactic: 0x0000000000000000, (Unnamed Layer* 203) [ElementWise]_out_tensor (Float[1,6,1,1]) -> output (Float[1,6])
Layer(Reformat): QuantizeLinear_2598, Tactic: 0x0000000000000000, 2704 (Float[1,2048,1,1]) -> 2706 (Int8[1,2048:4,1,1])
Layer(CaskConvolution): classfier2.weight + QuantizeLinear_2604 + transpose_before_Gemm_2608 + Gemm_2608 + classfier2.bias + (Unnamed Layer* 2499) [Shuffle] + unsqueeze_node_after_classfier2.bias + (Unnamed Layer* 2499) [Shuffle]_(Unnamed Layer* 2499) [Shuffle]_output + (Unnamed Layer* 2500) [ElementWise], Tactic: 0xc073b0053ce90eac, 2706 (Int8[1,2048:4,1,1]) -> (Unnamed Layer* 2500) [ElementWise]_out_tensor (Float[1,6,1,1])
Layer(NoOp): copied_squeeze_after_(Unnamed Layer* 2500) [ElementWise], Tactic: 0x0000000000000000, (Unnamed Layer* 2500) [ElementWise]_out_tensor (Float[1,6,1,1]) -> 2719 (Float[1,6])