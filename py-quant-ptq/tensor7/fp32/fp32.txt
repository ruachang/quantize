[12/22/2022-01:16:57] [V] [TRT] Engine Layer Information:
[12/22/2022-01:16:57] [V] [TRT] Layer(FusedConvActDirect): Conv_0 + Relu_1, Tactic: 8585215, input[Float(3,48,48)] -> 125[Float(64,48,48)]
[12/22/2022-01:16:57] [V] [TRT] Layer(FusedConvActDirect): Conv_2 + Relu_3, Tactic: 10682367, 125[Float(64,48,48)] -> 128[Float(64,48,48)]
[12/22/2022-01:16:57] [V] [TRT] Layer(Convolution): Conv_4 + Add_5 + Relu_6, Tactic: 118, 128[Float(64,48,48)], 125[Float(64,48,48)] -> 132[Float(64,48,48)]
[12/22/2022-01:16:57] [V] [TRT] Layer(FusedConvActDirect): Conv_7 + Relu_8, Tactic: 10682367, 132[Float(64,48,48)] -> 135[Float(64,48,48)]
[12/22/2022-01:16:57] [V] [TRT] Layer(Convolution): Conv_9 + Add_10 + Relu_11, Tactic: 118, 135[Float(64,48,48)], 132[Float(64,48,48)] -> 139[Float(64,48,48)]
[12/22/2022-01:16:57] [V] [TRT] Layer(Convolution): Conv_12 + Relu_13, Tactic: 57, 139[Float(64,48,48)] -> 142[Float(128,24,24)]
[12/22/2022-01:16:57] [V] [TRT] Layer(FusedConvActDirect): Conv_14, Tactic: 2424831, 142[Float(128,24,24)] -> 218[Float(128,24,24)]
[12/22/2022-01:16:57] [V] [TRT] Layer(scudnn): Conv_15 + Add_16 + Relu_17, Tactic: -4337126844824617177, 139[Float(64,48,48)], 218[Float(128,24,24)] -> 148[Float(128,24,24)]
[12/22/2022-01:16:57] [V] [TRT] Layer(FusedConvActDirect): Conv_18 + Relu_19, Tactic: 10682367, 148[Float(128,24,24)] -> 151[Float(128,24,24)]
[12/22/2022-01:16:57] [V] [TRT] Layer(Convolution): Conv_20 + Add_21 + Relu_22, Tactic: 6, 151[Float(128,24,24)], 148[Float(128,24,24)] -> 155[Float(128,24,24)]
[12/22/2022-01:16:57] [V] [TRT] Layer(Convolution): Conv_23 + Relu_24, Tactic: 1, 155[Float(128,24,24)] -> 158[Float(256,12,12)]
[12/22/2022-01:16:57] [V] [TRT] Layer(FusedConvActDirect): Conv_25, Tactic: 2424831, 158[Float(256,12,12)] -> 233[Float(256,12,12)]
[12/22/2022-01:16:57] [V] [TRT] Layer(Convolution): Conv_26 + Add_27 + Relu_28, Tactic: 1, 155[Float(128,24,24)], 233[Float(256,12,12)] -> 164[Float(256,12,12)]
[12/22/2022-01:16:57] [V] [TRT] Layer(FusedConvActDirect): Conv_29 + Relu_30, Tactic: 2424831, 164[Float(256,12,12)] -> 167[Float(256,12,12)]
[12/22/2022-01:16:57] [V] [TRT] Layer(Convolution): Conv_31 + Add_32 + Relu_33, Tactic: 118, 167[Float(256,12,12)], 164[Float(256,12,12)] -> 171[Float(256,12,12)]
[12/22/2022-01:16:57] [V] [TRT] Layer(Convolution): Conv_34 + Relu_35, Tactic: 57, 171[Float(256,12,12)] -> 174[Float(512,6,6)]
[12/22/2022-01:16:57] [V] [TRT] Layer(FusedConvActDirect): Conv_36, Tactic: 2424831, 174[Float(512,6,6)] -> 248[Float(512,6,6)]
[12/22/2022-01:16:57] [V] [TRT] Layer(Convolution): Conv_37 + Add_38 + Relu_39, Tactic: 0, 171[Float(256,12,12)], 248[Float(512,6,6)] -> 180[Float(512,6,6)]
[12/22/2022-01:16:57] [V] [TRT] Layer(FusedConvActDirect): Conv_40 + Relu_41, Tactic: 2424831, 180[Float(512,6,6)] -> 183[Float(512,6,6)]
[12/22/2022-01:16:57] [V] [TRT] Layer(Convolution): Conv_42 + Add_43 + Relu_44, Tactic: 1, 183[Float(512,6,6)], 180[Float(512,6,6)] -> 187[Float(512,6,6)]
[12/22/2022-01:16:57] [V] [TRT] Layer(Pooling): AveragePool_47, Tactic: -1, 187[Float(512,6,6)] -> 190[Float(512,1,1)]
[12/22/2022-01:16:57] [V] [TRT] Layer(CublasConvolution): Gemm_54, Tactic: 0, 190[Float(512,1,1)] -> (Unnamed Layer* 60) [Fully Connected]_output[Float(6,1,1)]
[12/22/2022-01:16:57] [I] Engine built in 41.0363 sec.
[12/22/2022-01:16:57] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.3.0 but loaded cuBLAS/cuBLAS LT 11.2.1
[12/22/2022-01:16:57] [V] [TRT] Allocated persistent device memory of size 36864
[12/22/2022-01:16:57] [V] [TRT] Allocated activation device memory of size 13275136
[12/22/2022-01:16:57] [V] [TRT] Assigning persistent memory blocks for various profiles